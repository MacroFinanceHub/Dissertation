\documentclass{pracamgr}
\usepackage[a4paper,pdftex]{geometry}                                                                           % A4paper margins
\setlength{\oddsidemargin}{5mm}                                                                                         % Remove 'twosided' indentation
\setlength{\evensidemargin}{5mm}
\usepackage[protrusion=true,expansion=true]{microtype}  
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{parallel,enumitem}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{amsmath, bm}
\usepackage{amsfonts}
\usepackage[T1]{fontenc}
\usepackage[polish,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{subfigure}
\usepackage{array}
\usepackage{etoolbox}
\usepackage{hyperref}
\usepackage{algorithm2e}
\usepackage[noend]{algpseudocode}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{caption} 
\captionsetup[table]{skip=10pt}
\captionsetup[algorithm2e]{skip=10pt}
\usepackage{booktabs,caption,fixltx2e}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{adjustbox}
\usepackage{lscape}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\usepackage{nameref}
\usepackage{tabularx}

\hypersetup{pdfstartview={XYZ null null 1.00}}

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\numberwithin{equation}{section}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{L}{>{\centering\arraybackslash}m{1.2cm}}
\newcolumntype{M}{>{\centering\arraybackslash}m{2.8cm}}

\lstset{ %
  language=MATLAB,                     % the language of the code
  basicstyle=\footnotesize,       % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},      % keyword style
  commentstyle=\color{ForestGreen},   % comment style
  stringstyle=\color{Orchid},      % string literal style
  escapeinside={\%*}{*)},         % if you want to add a comment within your code
  morekeywords={*,...}            % if you want to add more keywords to the set
} 

\lstset{ %
  language=python,                     % the language of the code
  basicstyle=\footnotesize,       % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},      % keyword style
  commentstyle=\color{ForestGreen},   % comment style
  stringstyle=\color{Orchid},      % string literal style
  escapeinside={\%*}{*)},         % if you want to add a comment within your code
  morekeywords={*,...}            % if you want to add more keywords to the set
} 


\author{Michał Miktus}

\nralbumu{11716114}

\title{Machine learning approach \\
to DSGE estimation}

\kierunek{Analysis and Policy in Economics}

\opiekun{Pablo Winant Ph. D.\\
  Paris School of Economics\\
  }

\date{June 2019}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}
\newtheorem{mydef}{Definition}

\begin{document}
\maketitle

 \begin{titlepage}
    
    \large
    \null
    \vfill
    
  \textbf{\Large Declaration of the supervisor} 
      \vspace{3mm}
      
   I declare the following thesis project was written under my supervision and I state that the project meets all submission criteria for the procedure of academic degree award.\\
   \vspace{5mm}
   
   Date \hfill Signature of the Supervisor
   
   \vspace{2cm}
   \textbf{\Large Declaration of the author of the project}
    \vspace{3mm}
    
   Aware of legal responsibility, I declare that I am the sole author of the following thesis project and that the project I submit is entirely free from any content that constitutes copyright infringement or has been acquired contrary to applicable laws and regulations.\\
   
   I also declare that the below project has never been subject of degree-awarding procedures in any school of higher education.\\

   Moreover I declare that the attached version of the thesis project is identical with the enclosed electronic version.\\
   \vspace{5mm}
   
   Date  \hfill Signature of the Author 
    \vspace{3cm}
 
\end{titlepage}
    
\begin{abstract}
The estimation of Dynamic Stochastic General Equilibrium (DSGE) models through the Maximum Likelihood has a relatively protracted history. Elseways, the Machine Learning (ML) frameworks recently, yet profoundly entered the economics field. The following paper attempts to implement the advanced achievements of ML, not to mention the Automatic Differentiation (AD) or Stochastic Gradient Descent (SGD), to the Maximum Likelihood estimation of renowned \citet{smets2003estimated} DSGE model. The author identifies that the proposed framework can be considered as a valuable alternative to the Bayesian interference in case of highly-dimensional space parameter estimation.
\end{abstract}

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms


\chapter*{Introduction} \label{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Machine learning is an application of artificial intelligence (AI) that provides systems the capability to automatically lucubrate and progress from experience without being explicitly programmed. It concentrates on the development of computer programs being able to access data and then use it in their learning process. In the last decade, machine learning frameworks made the vast progress in plethora of fields, gradually entering into the economics applications as well (\citet{mullainathan2017machine}). 

On contrary, Dynamic Stochastic General Equilibrium (DSGE) models have become a standard tool in macroeconomics studies. Their attractiveness arises from the explicit specification of the objectives and constraints faced by households and firms, further employed in determination of the prices and allocations that emerge from their market interaction in an uncertain environment. Until recently, the calibration was the most common concept in the literature to explore the empirical properties of DSGE models. To be more specific, the calibration consists of fixing the structural parameters values to those estimated in preceding studies or those calculated using long-run averages of aggregate data. 

In spite of the fact that the calibration is an appreciated utensil for understanding the dynamic properties of DSGE models, there is a plethora of advantages in their fully-fledged econometric estimation. To begin with, parameter estimates are obtained by imposing the restrictions of the model on the data, therefore they can overcome the inconsistency of DSGE assumptions with the calibration assumptions from the micro studies. Furthermore, the econometric estimation of the DSGE model permits to obtain estimates of parameters that may be difficult or even impossible to calculate from disaggregated data. Additionally, bootstrap techniques can be incorporated explicitly in impulse-response analysis, allowing to construct confidence intervals for the model’s response to a shock, thus integrating the parameter uncertainty. Finally, classic tools of model selection and evaluation can be promptly enforced.

The Maximum Likelihood estimation procedure is relatively standard and its asymptotic properties are well known. Moreover, there are several studies implementing the aforementioned framework to the DSGE estimation, not to mention \citet{ruge2007methods} or \citet{andreasen2010maximize}. The main contribution of the following paper is to capitalize the recent advancements in the machine learning field in order to enhance the Maximum Likelihood estimation, not to mention the tensor platforms in the form of deep-learning libraries, exploiting the Automatic Differentiation advantages.

Performed Monte Carlo analysis clearly indicates that the proposed framework can be deliberated as a profitable substitute to the Bayesian interference in case of highly-dimensional space parameter estimation. On the other hand, the implemented algorithms are surprisingly costly, in both time and memory allocation, in the case of solitary parameter estimation.

The paper is organized as follows, Chapter 1 exhaustively describes the machine learning, while the next one illustrates the DSGE model that will serve as backdrop for the estimation procedures.. Next section characterizes the solution techniques, while estimation procedures and their application to DSGE models are outlined in chapter 4. Section 5 presents the Monte Carlo design and report its results. The dissertation is completed with the concluding remarks with potential extensions, references and appendices with codes in Dynare and Python.

\chapter{Machine learning} \label{MachineLearning}

Machine learning (ML) permits to perform analysis of tremendous quantities of data. Moreover, it generally delivers faster and more accurate results, simultaneously requiring additional time and resources to be trained accurately. Hence, based on the assessment of its early contributions to economics, it is predicted to have a dramatic impact on the social sciences field within a foreseeable future (\citet{athey2018impact}). In the following section the definition of machine learning is granted, with a brief description of its main strengths, as well as fundamental weaknesses, contrasting them with traditional and widely implemented econometric and statistical tools. In addition, the automatic differentiation procedure is thoroughly expounded, with the emphasis imposed on its contributions to the ML optimization methods, further augmented with two elementary illustrations of forward and backward automatic differentiation techniques. Finally, as the ML framework requires a proper ecosystem of tensor computations such as PyTorch or TensorFlow, both of the above-mentioned deep learning platforms are briefly explained.

\section{Definition}

It has to be underlined at the beginning that it does not exist single and generally acceptable definition of machine learning. In the broad understanding, it refers to a collections of subfields of computer science, as well as a set of topics developed and used across computer science, engineering, statistics, and increasingly the social sciences. In the aforementioned meaning, ML is an application of artificial intelligence (AI) that provides systems able to automatically peruse and progress from experience without being explicitly programmed. On contrary, conferring to the narrower definition, machine learning is a field that develops algorithms designed to be applied to datasets, with the main areas of focus being optimization,  prediction in both regression and classification instances, and clustering or grouping tasks.

Furthermore, machine learning tasks can be classified into several broad categories. As far as the supervised learning is concerned, the algorithm constructs a mathematical model from a data containing both the inputs and the desired outputs. Oppositely, in unsupervised learning, a mathematical model is built from a set of data which encloses only inputs, without any output labels. In other words, it involves finding clusters of observations that are similar in terms of their covariates and thus can be viewed as \textit{dimensionality reduction}. In addition, semi-supervised learning algorithms lay in-between and develop mathematical models from incomplete training data, where a chunk of the sample input lacks labels. What is more, the reinforcement machine learning algorithms interact with its environment by producing actions and discover errors or rewards, automatically determining the ideal behavior within a specific context in order to maximize its performance. Finally, the main study of the following paper is the last wide ML application: the optimization.

Machine learning intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples, which in turn expresses the divergence between the predictions of the trained model and the actual problem instances. Although it is true that machine learning optimization is a subset of optimization, there are some differences between classical optimization and machine learning (\citet{goodfellow2016deep}). Namely, in most machine learning scenarios, the main concern is imposed on some performance measure, defined with respect to the test set and possibly intractable. In order to optimize it, a different cost function is introduced and then minimized, expecting that it will lead to the performance measure improvement.This stays in contrast to pure optimization, where minimizing a cost function is a goal in and of itself.

\section{Characteristics}

Machine learning and econometrics share purpose and a philosophical underpinning, incorporating data and background knowledge to induce new expertise. Such new, inferred proficiency can specify \textit{in-sample} data generalizations (referred to as description or identification) or forecast and extrapolate into \textit{out-of-sample} data generalizations, such as patterns or trends. To infer sound generalizations out of data both seek statistical support in the statistics body of knowledge, mainly statistic inference theory. Thus both are subject to bias, both are prone to overfitting, both suffer equally from bad quality input data. Therefore, data cleaning and preparation is essential for both to produce useful, actionable insights.

What distinguishes them is mainly scope. Econometric body of knowledge was developed and fitted to address economic (financial) data and questions. Machine learning is broader and from on-set has no commitment or attachment to any particular field of application. Hence, econometrics tends to incorporate more easily economic background knowledge, vide semi- and structured models, whereby economy theory (or expertise, or intuition) is explicitly represented and taken into account to estimate and to forecast.

On the other hand, most machine learning techniques, in particular, the most used and successful ones, cannot cater for background knowledge as swiftly and as neatly. In fact, background knowledge is more incorporated in the data preparation phase, as well as in heuristics and in technique specific parametrization such as structural topology, thresholds or pruning decisions.

Finally, it is necessary to emphasis one particular gain for economic modeling and analysis that hails from machine learning application: a whole infrastructure to account for highly non-linear relationships. On the other hand, the conundrum between correlation and causation is more effectively unraveled in the econometric field (\citet{boelaert2018great}).

Before proceeding, it is useful to highlight two other contributions of the ML literature. The first one is computational rather than conceptual, but it has had such a large impact that it merits a short discussion. The technique is called stochastic gradient descent (SGD, \citet{bottou2010large}) and it meticulously explained in the chapter \ref{Estimation}. The second benefaction, the automatic differentiation, is the topic of the next section.

\section{Automatic differentiation}

The maximization of the likelihood function can be performed through standard optimization procedures such as L-BFGS (\citet{zhu1997algorithm}) or stochastic gradient descent (\citet{bottou2010large}), both of them involving the calculation of derivatives. In fact, derivatives, predominantly in the form of gradients and Hessians of the objective function, are omnipresent in machine learning (\citet{sra2011introduction}). Techniques for the computation of derivatives in programming languages can be classified into four main categories: (1) manually deriving derivatives and coding them; (2) numerical differentiation using finite difference approximations; (3) symbolic differentiation encompassing the expression manipulation in specifically designed computer algebra systems such as Mathematica, Maxima, and Maple; (4) automatic differentiation (AD), also called algorithmic differentiation or autodiff, which is the fundamental subject of this dissertation. 

Manual differentiation is a tedious, time consuming and prone to error procedure, requiring a model to be defined as a closed-form expression, eliminating or severely confining algorithmic control flow and expressivity. Alternatively, numerical differentiation is relatively straightforward to implement but due to round-off and truncation errors it can be eminently inaccurate (\citet{jerrell1997automatic}) and unsatisfactorily scalable. On the other hand, symbolic differentiation sermonizes the defficiencies of both the manual and numerical methods, but often results in complex, cryptic and similarly to the manual technique closed-form expressions.

The fourth technique, automatic differentiation, surmounts all of the above-mentioned obstacles, with its basic form consisting of applying symbolic differentiation at the elementary operation level and keeping intermediate numerical results, in lockstep with the evaluation of the main function (\citet{baydin2018automatic}). It can be proved that all numerical computations are ultimately compositions of a finite set of elementary operations for which derivatives are known (\citet{griewank2008evaluating}). One can enumerate for instance the binary arithmetic operations, the unary sign switch or transcendental functions, not to mention the exponential, the logarithm or the trigonometric functions. Moreover, the derivative of the overall composition can be easily derived from the chain rule, imposed on the derivatives of the constituent operations.

A substantial point to emphasis is that AD can differentiate not only closed-form expressions in the classical sense, but the algorithms using control flow such as branching, loops, recursion and procedure calls as well. It will be essential for the looping structure of the Linear Time Iteration algorithm, discussed in section \ref{LTI}, for which the automatic differentiation seems like an indispensable tool in further maximization process.

\subsection{Computational graph}

The basic mathematical ideas behind AD have been known for a long period of time. However, interest in AD had definitely revived by 80s' due to improvements in programming techniques and the introduction of the efficient reverse mode. A useful tool in the development of AD following 1980 was the computational graph, which is a way of visualizing an algorithm or computer program different from formulas. For instance, Fig. \ref{Computational_graph} illustrates a computational graph, technically known as a directed acyclic graph (DAG). Transformations of the algorithm such as differentiation in forward or reverse mode can be thus expressed as modifications of the computational graph.

\begin{figure}[H] \centering
\includegraphics[scale=0.3]{Computational_graph}
\caption[Computational graph]{Computational graph\footnotemark}
\label{Computational_graph}
\end{figure} 

\footnotetext{Source: \citet{bucker2006automatic}}

Paraphrasing it, a computation graph is a DAG in which nodes represent variables, such as matrices, scalars or tensors, while the edges depict some mathematical operations, for instance summation or multiplication. Thus, during the optimization step, the chain rule and the graph are combined in order to calculate the derivative of the output with respect to the learnable variable.

There are two different modes of automatic differentiation, based on the application of the chain rule. The next section will start with forward mode automatic differentiation, which often is find a little more intuitive, described on the simple example.

\subsection{Forward mode}

Given a function $f$, one can construct a computational graph (a directed acyclic one) representing it, for instance let:

\[ f(x,y) = \cos(x) \sin(y) + \dfrac{x}{y} \]

As mentioned previously, each node of the graph expresses a primitive function, while the edges enact the flow of information:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth,height=0.8\textheight,keepaspectratio]{Aut_diff}
\caption[Forward propagation]{Forward propagation\footnotemark}
\label{Forward_propagation}
\end{figure}

\footnotetext{Source: \url{http://www.columbia.edu/~ahd2125/post/2015/12/5/}}

Forward differentiation operates by recursive computation of derivatives of nodes in terms of their parents. Suppose one wants to calculate the partial $\dfrac{\partial f}{\partial x}$, denoted by $Df$. Then:

\begin{align*}
& Df = Dw_{7} = D(w_{5} + w_{6}) = Dw_{5} + Dw_{6} \\
& Dw_{6} = D\dfrac{w_{1}}{w_{2}} = \dfrac{w_{1}Dw_{2}- w_{2}Dw_{1}}{w_{2}^{2}}  \\
& Dw_{5} = Dw_{3}w_{4} = w_{3}Dw_{4} + w_{4}Dw_{3} \\
& Dw_{4} = D\sin(w_{2}) = \cos(w_{2}) Dw_{2} \\
& Dw_{3} = D\cos(w_{1}) = -\sin(w_{1}) Dw_{1} \\
& Dw_{2} = Dy \\
& Dw_{1} = Dx 
\end{align*}

The final value of $Df$ depends only on $x, y, Dw_{1}, Dw_{2}$. In this case, $ D = \dfrac{\partial f}{\partial x}$, so $Dx = 1$ and $Dy = 0$. Because information flows in the same direction as when one evaluates the expression (bottom-up), it is called the \textit{forward mode}. Predictably, the information flows top-down in  \textit{backward mode}, also called \textit{reverse mode}.

\subsection{Backward mode}

Reverse mode automatic differentiation lets calculate gradients much more efficiently than forward mode automatic differentiation. Suppose one wants to calculate the gradient of the $f$ function described in the previous section:

\[ \triangledown f = \left<\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right> = \left<\frac{\partial w_{7}}{\partial w_{1}}, \frac{\partial w_{7}}{\partial w_{2}}\right> \]

Then:

\begin{align*}
& \frac{\partial w_{7}}{\partial w_{1}} = \frac{\partial w_{3}}{\partial w_{1}} \frac{\partial w_{7}}{\partial w_{3}} + \frac{\partial w_{6}}{\partial w_{1}} \frac{\partial w_{7}}{\partial w_{6}} = -\sin w_{1} \frac{\partial w_{7}}{\partial w_{3}} + \frac{1}{w_{2}} \frac{\partial w_{7}}{\partial w_{6}} \\
& \frac{\partial w_{7}}{\partial w_{2}} = \frac{\partial w_{4}}{\partial w_{2}} \frac{\partial w_{7}}{\partial w_{4}} + \frac{\partial w_{6}}{\partial w_{2}} \frac{\partial w_{7}}{\partial w_{6}} = \cos w_{2} \frac{\partial w_{7}}{\partial w_{4}} - \frac{w_{1}}{w_{2}^{2}} \frac{\partial w_{7}}{\partial w_{6}} \\
& \frac{\partial w_{7}}{\partial w_{3}} = \frac{\partial w_{5}}{\partial w_{3}} \frac{\partial w_{7}}{\partial w_{5}} = w_{4} \frac{\partial w_{7}}{\partial w_{5}} \\
& \frac{\partial w_{7}}{\partial w_{4}} = \frac{\partial w_{5}}{\partial w_{4}} \frac{\partial w_{7}}{\partial w_{5}} = w_{3} \frac{\partial w_{7}}{\partial w_{4}} \\
& \frac{\partial w_{7}}{\partial w_{5}} = 1 \\
& \frac{\partial w_{7}}{\partial w_{6}} = 1
\end{align*}

Instead of implementing the chain rule to get derivatives of parents in terms of their children, one proceeds fully oppositely, computing derivatives of children nodes in terms of their parents. Because information flows top-down, the reverse of normal evaluation, it is called the \textit{reverse mode} automatic differentiation.

Notice that  all operations in reverse mode differentiation are scalar operations, even though one calculates the (vector) gradient. Computing gradients via reverse mode can be thus significantly faster if $f$ contains a vast number of input variables.

\section{Tensor platform}

PyTorch is a relatively new deep learning library, backing dynamic computation graphs and tensors which can tap into the resources of a GPU to significantly speed up matrix operations. The dynamic graph paradigm allows to make changes to the model architecture during runtime, as a graph is created only when a piece of code is run. This means a graph may be redefined during the lifetime for a program. This, however, is not possible with static graphs, used by another popular deep learning libraryTensorFlow, where graphs are created before running the program, and merely executed later. 

Finally, PyTorch has in-built several optimization algorithms, such as Stochastic Gradient Descent or ADAM, which solves the likelihood maximization problem relatively costlessly.

\subsection{GPU accelaration}

A central processing unit (CPU) is essentially the brain of any computing device, effectuating the instructions of a program by enforcing control, logical, and input/output (I/O) operations. Today’s CPUs have just a handful of cores, and its basic design and purpose—to process complex computations—has not really changed over the years. Essentially, they are mostly applicable for problems that require parsing through or interpreting complex logic in code. A graphical processing unit (GPU), on the other hand, has smaller-sized but many more logical cores (arithmetic logic units or ALUs, control units and memory cache) whose basic design is to process a set of simpler and more identical computations in parallel. As a result, GPU can highly accelerate the Automatic Differentiation, in fact composed of simple matrix math calculations. 

Both aforementioned deep learning libraries enable the GPU computation, therefore it can considered as valuable extension.

\chapter{Dynamic Stochastic General Equilibrium (DSGE) model} \label{Dynamic Stochastic General Equilibrium (DSGE) model}

It is generally acknowledged that a satisfactory macroeconomic model should be grounded on rational intertemporal optimizing behavior of microagents, satisfying the rational expectations assumption. Furthermore, it should reflect the imperfectly flexible wages and prices, associated with deficiencies on labor and product markets. One of the most renowned New Keynesian model satisfying the above-mentioned presumptions was developed by \citet{smets2003estimated}, as the elaboration of previous work of \citet{christiano2001nominal}.

Succinctly, it assumes that in the closed economy framework there exists a continuum of households, who supply household-specific labor in monopolistically competitive environment. In addition, they set Calvo-sticky wages, deriving utility from leisure and consumption in relation to their habits. Households save and part of their savings is transformed into capital through a household technology, which in turn has its own adjustment costs. Moreover, they rent out their capital to the intermediate-good sector and choose total amount of  investment with variable capital utilization rate. The rest of their savings is spent for lending and borrowing decisions, not to mention the bond holding settlements.

On the firm side, a continuum of intermediate goods produced by firms in monopolistic competition is assumed. Their production requires various kinds of differentiated labor and (homogeneous) capital, leading to the Calvo-sticky prices setting. On the other hand, final goods are produced in perfect competition framework through intermediate goods and they are consumed by households and government, allowing for the fiscal policy and Taylor-type monetary policy rule.

Finally, there is a plethora of sources of shocks, such as two preference shocks, aggregate productivity shock or monetary policy rule shock (similarly to the \citet{christiano2005nominal}).

After introduction of the main essence of the \citet{smets2003estimated}  model, in the following sections it will be derived meticulously.

\section{Households} \label{DSGE - Households}

In the examined economy, there is a continuum of mass one households indexed by $\tau \in (0,1)$, maximizing an intertemporal utility function over an endless horizon:

\begin{equation}
 \mathbb{E}_{0} \left(\sum\limits_{t=0}^{\infty} \beta^{t} U_{t}^{\tau} \right) 
\end{equation}

where $\beta$ is the discount factor and $U_{t}^{\tau}$ stands for the contemporaneous utility
function depending on consumption $C_{t}^{\tau}$, and labor $l_{t}^{\tau}$.

The utility function is separable in its two arguments:

\begin{equation}
U_{t}^{\tau} \left( C_{t}^{\tau}, l_{t}^{\tau} \right) = \varepsilon^{B}_{t} \left[ \frac{1}{1-\sigma_{c}} \left( C_{t}^{\tau} - H_{t} \right)^{1-\sigma_{c}} - \frac{\varepsilon_{t}^{L}}{1+\sigma_{l}} \left( l_{t}^{\tau} \right)^{1+\sigma_{l}} \right]
\end{equation}

where $\sigma_{c}$ is the measure of relative risk aversion or the inverse of the intertemporal elasticity of substitution, while $\frac{1}{\sigma_{l}}$ denotes the elasticity of labor with respect to the real wage.

The above-mentioned utility function includes two shocks, namely a general preference shock $\varepsilon^{B}_{t}$ affecting intertemporal substitution of households and a labor supply shock $\varepsilon^{L}_{t}$. All of them follow autoregressive processes of the first order with normally and independently distributed error terms:

\begin{align}
& \varepsilon^{B}_{t} = \rho_{B} \cdot \varepsilon^{B}_{t-1}+ \eta_{t}^{B}, \enspace \enspace \eta_{t}^{B} \sim \mathcal{N}(0,1) \\
& \varepsilon^{L}_{t} = \rho_{L} \cdot \varepsilon^{L}_{t-1}+ \eta_{t}^{L}, \enspace \enspace\enspace \eta_{t}^{L} \sim \mathcal{N}(0,1) 
\end{align}

The utility function includes as well a habit $H_{t}$ that is proportional to the aggregate past consumption:

\begin{equation}
H_{t} = h \cdot C_{t-1}
\end{equation}

which allows the optimal behavior of households to respond to shocks with an hump-shaped and lagged path of the optimal consumption. It has to be underlined that the habit variable depends on lagged aggregate consumption that is unaffected by any one agent's decisions.

Furthermore, the income of the households $Y_{t}^{\tau}$ takes the form of:

\begin{equation}
Y_{t}^{\tau} = \frac{W_{t}^{\tau}}{P_{t}} \cdot l_{t}^{\tau} + A_{t}^{\tau} + D_{t}^{\tau} + \left( r_{t}^{k} z_{t}^{\tau} - \Psi(z_{t}^{\tau}) \right) K_{t-1}^{\tau} - T_{t}
\end{equation}

thus depending on real labor income $l_{t}^{\tau} \cdot \frac{W_{t}^{\tau}}{P_{t}} $, net cash inflow from participating in state-contingent securities $A_{t}^{\tau}$, the dividends derived from the imperfect competitive intermediate firms $D_{t}^{\tau}$, taxes $T_{t}$ and the return on the real capital stock depreciated by the cost associated with variations in the degree of capital utilization $z_{t}^{\tau}$. It is assumed that the cost of capital utilization is zero when capital utilization is one ($\Psi(1) = 0$) as it would in the steady state. Framing differently, households rent capital services to firms and decide about the amount of capital to accumulate given certain capital adjustment costs. With the augmentation of the rental price of capital, the capital stock can be used more intensively according to a specific cost schedule.

Households hold their financial wealth in the form of bonds $B_{t}$, which are one-period securities with market price $b_{t}$ and gross return:

\begin{equation}
R_{t} = 1 + i_{t}= \frac{1}{b_{t}}
\end{equation}

Current income and financial wealth can be used for consumption and investment in physical capital, thus households maximize their objective function subject to an intertemporal budget constraint:

\begin{equation}
C_{t}^{\tau} + I_{t}^{\tau} + b_{t} \frac{B_{t}^{\tau}}{P_{t}} = Y_{t}^{\tau} + \frac{B_{t-1}^{\tau}}{P_{t}}
\end{equation}

Finally, following \citet{christiano2005nominal}, it is assumed that there exist state-contingent securities that perfectly insure the households against variations in household specific labor income. As a result, the labor income plus the net cash-inflow from participating in state-contingent securities component $\frac{W_{t}^{\tau}}{P_{t}} \cdot l_{t}^{\tau} + A_{t}^{\tau}$ in the household's income will be equal to aggregate labor income. Analogously the consumption of an individual household will equal the aggregate consumption and the marginal utility of consumption will be identical across different types of households. Consequence, capital holdings $K_{t}^{\tau} \equiv K_{t}$, bond holdings $B_{t}^{\tau} \equiv B_{t}$ as well as firm dividends  $D_{t}^{\tau} \equiv D_{t}$ will be indistinguishable across different types of households.

\subsection{Consumption and savings behavior} \label{Consumption and savings behavior}

The maximization of the objective function of the household subject to the budget constraint with respect to consumption and holdings of bonds, may be performed through the Lagrangian-based technique, yielding the first-order condition for consumption growth in the existence of external habit formation:

\begin{align}
\mathbb{E}_{t} \left[ \beta \frac{\varepsilon_{t+1}^{B} (C_{t+1}^{\tau} - h \cdot C_{t} )^{-\sigma_{c}}}{\varepsilon_{t}^{B} (C_{t}^{\tau} - h \cdot C_{t-1} )^{-\sigma_{c}}} R_{t} \frac{P_{t}}{P_{t+1}} \right] = 1
\end{align}

or equivalently:

\begin{align}
\mathbb{E}_{t} \left[ \beta \frac{\lambda_{t+1}}{\lambda_{t}} R_{t} \frac{P_{t}}{P_{t+1}} \right] = 1
\end{align}

where:

\begin{align}
\lambda_{t} = \frac{\partial U_{t}^{\tau}}{\partial C_{t}^{\tau}} = \varepsilon_{t}^{B} (C_{t}^{\tau} - H_{t} )^{-\sigma_{c}} = \varepsilon_{t}^{B} (C_{t}^{\tau} - h \cdot C_{t-1} )^{-\sigma_{c}} = \varepsilon_{t}^{B} (C_{t} - h \cdot C_{t-1} )^{-\sigma_{c}}
\end{align}

The exact calculations for the whole chapter's derivations are performed in the \nameref{Appendix A}.

\subsection{Labor supply decisions and the wage setting equation} \label{Labor Supply decisions and the wage setting equation}

Households differ, supply ing a differentiated type of labor. Thus, each household has a monopoly power over the supply of its labor. In other words, labor is a differentiated good, supplied by households in a monopolistic competition to the production sector. Consequently, firms consider labor supplies $l_{t}^{\tau}$ as imperfect substitutes. The aggregate labor demand $L_{t}$ and the aggregate nominal wage $W_{t}$ are given by the following Dixit-Stiglitz type aggregator, where $\lambda_{w ,t} > 0$ is the substitution parameter:

\begin{align}
L_{t} &= \left[ \int\limits_{0}^{1} \left( l_{t}^{\tau} \right)^{\frac{1}{1+\lambda_{w,t}}} d\tau \right]^{1+\lambda_{w,t}}
\end{align}

It can be observed that the condition $\lambda_{w ,t} = 0$ would correspond to the perfect substitution of labor.

Therefore, firms minimize the cost function $\int\limits_{0}^{1} W_{t}^{\tau} l_{t}^{\tau} d\tau$ under the aggregate labor demand, which in turn yields:

\begin{align} \label{Aggregate labor demand}
L_{t} &= \left[ \int\limits_{0}^{1} \left( \frac{W_{t}^{\tau}}{W_{t}}\right)^{\frac{-1}{\lambda_{w ,t}}} L_{t}^{\frac{1}{1+\lambda_{w ,t}}} d\tau \right]^{1+\lambda_{w,t}} \nonumber \\
& \iff W_{t} = \left[ \int\limits_{0}^{1} \left( W_{t}^{\tau} \right)^{\frac{-1}{\lambda_{w ,t}}}  d\tau \right]^{-\lambda_{w,t}}
\end{align}

where the Lagrange multiplier $W_{t}$ can be interpreted as well as the price of a working hour and thus a wage index.

In addition, the determination of wages follows a Calvo rule. In other words, each household $\tau$ has a (constant) probability of $1-\varsigma_{w}$ in every period $t$ to be able to set its nominal wage and therefore in each period $t$ a fraction of $1-\varsigma_{w}$ households adapts its wage. Moreover, wages of the fraction of households $\varsigma_{w}$ that cannot reoptimize are partially anchored to the inflation rate:

\begin{align}
W_{t}^{\tau} = \left( \Pi_{t-1} \right)^{\gamma_{w}}W_{t-1}^{\tau} = \left( \frac{P_{t-1}}{P_{t-2}} \right)^{\gamma_{w}}W_{t-1}^{\tau}
\end{align}

where $\gamma_{w}$ is the degree of wage indexation, such that $\gamma_{w} = 0$ entails no indexation ($W_{t}^{\tau} = W_{t-1}^{\tau}$), while $\gamma_{w} = 1$ suggests a complete indexation ($W_{t}^{\tau} = \Pi_{t-1} W_{t-1}^{\tau}$).

After denoting by $\widetilde{W}_{t}$ the wage of the household which is able to reoptimize in period $t$, it follows:

\begin{align}
W_{t}^{\tau} = \begin{cases}
\widetilde{W}_{t} & \text{with probability} \enspace 1-\varsigma_{w} \\
\left( \Pi_{t-1} \right)^{\gamma_{w}}W_{t-1}^{\tau} & \text{with probability} \enspace \varsigma_{w} 
\end{cases}
\end{align}

Therefore, the motion for the aggregate wage index $W_{t}$ equation can be computed through \ref{Aggregate labor demand}:

\begin{align}
\left(W_{t}\right)^{\frac{-1}{\lambda_{w,t}}} &= \varsigma_{w} \cdot \int\limits_{0}^{1} \left( W_{t-1}\left( \frac{P_{t-1}}{P_{t-2}} \right)^{\gamma_{w}} \right)^{\frac{-1}{\lambda_{w,t}}} d\tau + (1 - \varsigma_{w}) \cdot \int\limits_{0}^{1} \widetilde{W}_{t}^{\frac{-1}{\lambda_{w,t}}} d\tau = \nonumber \\ 
& = \varsigma_{w}\cdot \left[ W_{t-1}\left( \frac{P_{t-1}}{P_{t-2}} \right)^{\gamma_{w}} \right]^{\frac{-1}{\lambda_{w,t}}} + (1 - \varsigma_{w}) \cdot \widetilde{W}_{t}^{\frac{-1}{\lambda_{w,t}}}
\end{align}

The probability for $\widetilde{W}_{t}$ not to re-set until period $i$ is $\left( \varsigma_{w} \right)^{i}$, thus through the indexation the not reoptimized wage in period $t+i$ becomes:

\begin{align}
W_{t}^{\tau} &= \widetilde{W}_{t} \nonumber \\
W_{t+1}^{\tau} &= \left( \frac{P_{t}}{P_{t-1}} \right)^{\gamma_{w}} W_{t}^{\tau} = \left( \frac{P_{t}}{P_{t-1}} \right)^{\gamma_{w}} \widetilde{W}_{t}  \nonumber \\
W_{t+2}^{\tau} &= \left( \frac{P_{t+1}}{P_{t}} \right)^{\gamma_{w}}  W_{t+1}^{\tau} = \left( \frac{P_{t+1}}{P_{t}} \right)^{\gamma_{w}} \left( \frac{P_{t}}{P_{t-1}} \right)^{\gamma_{w}}  W_{t}^{\tau}  = \left( \frac{P_{t+1}}{P_{t-1}} \right)^{\gamma_{w}} \widetilde{W}_{t} \nonumber \\ \vdots \nonumber \\
W_{t+i}^{\tau} &= \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \widetilde{W}_{t} \nonumber \\
\end{align}

Resetting-wages households maximize their objective function subject to their budget constraint and the demand of labor, taking into consideration the fact that wages remain fixed until period $i$ with a probability $\left( \varsigma_{w} \right)^{i}$, which yields:

\begin{align}
\frac{\widetilde{W}_{t}}{P_{t}} &\mathbb{E}_{t} \left\{ \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{w}^{i} \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}}  \frac{P_{t}}{P_{t+i}} \left( \frac{l_{t+1}^{\tau} U_{t+i}^{C}}{1 + \lambda_{w, t+i}} \right) \right\} = \nonumber \\
&= \mathbb{E}_{t} 
\left\{ \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{w}^{i} l_{t+1}^{\tau} U_{t+i}^{l} \right\}
\end{align}

Assuming perfect flexibility of wages $(\varsigma_{w} = 0)$, the above-mentioned equation becomes:

\begin{align}
\frac{\widetilde{W}_{t}}{P_{t}} = \left(1 + \lambda_{w, t} \right)\frac{U_{t}^{L}}{U_{t}^{C}}
\end{align}

It will be moreover assumed that the degree of substitutability $\lambda_{w, t}$ is affected by a normally and independently distributed shocks:

\begin{align}
\lambda_{w, t} = \lambda_{w} + \eta_{t}^{w}, \quad \eta_{t}^{w} \sim \mathcal{N} \left(0, \sigma^{2}_{\eta_{w}} \right)
\end{align}

In the flexible-wage economy, $1+\lambda_{w, t}$ will be the markup of real wages over the classical ratio of the marginal disutility of labor to the marginal utility of consumption. Thus, $\eta_{t}^{w}$ can be regarded as a wage markup shock.

\subsection{Capital accumulation and investment decision} \label{Capital accumulation and investment decision}

Households are the proprietors of the stock of capital of the economy and rent it at the rate $r_{t}^{k}$ to the firms of the intermediate sector. Moreover, investment in period $t-1$ increases the stock of capital in period $t$ and households are assumed to choose the utilization rate $z_{t}$ of their stock of capital. The equation of motion of capital becomes:

\begin{align}
K_{t} = K_{t-1} (1 - \tau) + I_{t} \left[ 1 - S \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \right]
\end{align}

where $\tau$ is the depreciation rate, while the function $S$ introduces in the model the adjustment costs for investment, which in turn are contingent on its rate of growth, proposed in order to avoid strong oscillations of capital. It will be  furthermore assumed that for the constant growth rate of investment, as in the steady state, $S(1)=0$ and similarly in the neighborhood of the steady state $S'(1) = 0$. Thus, adjustment costs depend solely on the second derivative such that $S''(1)>0$. Finally, the function $S$ is subject to a shock $\varepsilon_{t}^{I}$ following an $AR(1)$ process:

\begin{align}
\varepsilon_{t}^{I} = \rho_{I} \varepsilon_{t-1}^{I} + \eta_{t}^{I}, \quad  \eta_{t}^{I} \sim \mathcal{N} \left(0,1\right)
\end{align}

Solving the respective maximization problem with the Lagrange multiplier in the form of $\lambda_{t} Q_{t}$ yields:

\begin{align}
&r_{t}^{k} = \Psi'(z_{t})  \\
&Q_{t} = \mathbb{E}_{t} \left[ \beta \frac{\lambda_{t+1}}{\lambda_{t}} \left( Q_{t+1}(1-\tau) + z_{t+1}r_{t+1}^{k} - \Psi(z_{t+1}) \right) \right]  \\
&\mathbb{E}_{t} \left[ -\beta^{t} \lambda_{t} - \beta^{t} \lambda_{t} Q_{t} \left(-1 + S\left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) + I_{t} \cdot S' \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \cdot \frac{\varepsilon_{t}^{I}}{I_{t-1}} \right) \right] = \nonumber \\
\qquad &= \mathbb{E}_{t} \left[ \beta^{t+1} \lambda_{t+1} Q_{t+1} \left(I_{t+1} \cdot S' \left(\frac{\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}} \right) \cdot \frac{-\varepsilon_{t+1}^{I} I_{t+1}}{I_{t}^{2}} \right) \right]
\end{align}

In other words the capital utilization rate is set so that the revenue $r_{t}^{k}$ of the marginal utilization equals the marginal costs $\Psi'(z_{t})$, the today's real value of the stock of capital $\lambda_{t} Q_{t}$ is balanced with the expected value of sum of not depreciated stock of capital of the next period $\lambda_{t+1}Q_{t+1}(1-\tau)$ and the expected revenue of the future utilization $ z_{t+1}r_{t+1}^{k}$ minus the related costs $\Psi(z_{t+1})$, while the costs of the marginal investment (included the adjustment costs) have to be identical to the expected marginal revenue of investment.

Additionally, $\lambda_{t} Q_{t}$ can be regarded as the marginal value of capital. Thus, basing on the fact that $\lambda_{t}$ break evens the marginal utility of consumption, $Q_{t}$ can be viewed as marginal Tobin-Q (the ratio of the marginal value of capital by virtue of the increment of the capital stock over the marginal opportunity cost):

\begin{align}
Q_{t} = \frac{\lambda_{t} Q_{t}}{\lambda_{t}}
\end{align}

At the steady-state $Q=1$, thus marginal revenues and costs mutually reimburse. If $Q>1$ there are enticements to upturn the capital accumulation through investment, while if $Q<1$ the reverse holds true.

\section{Firms} \label{DSGE - Firms}

The economy produces only one final product $Y_{t}$ and a continuum of intermediates goods defined over $j \in [0,1]$, such that firm $j$ produces the intermediate product $y_{t}^{j}$ . Firms of the final product sector operate in a (perfect) competitive market, while firms in the intermediate sector function in a monopolistic competition and set their prices forward-looking.

\subsection{Final product sector} \label{DSGE - Firms- Final product sector}

For the production of the final product $Y_{t}$ firm use the intermediate products $y_{t}^{j}$ as input, with aggregation performed through the following technology:

\begin{align} \label{Technology constraint}
Y_{t} = \left[ \int\limits_{0}^{1} \left(y_{t}^{j} \right)^\frac{1}{1+\lambda_{p,t}} dj \right]^{1+\lambda_{p,t}}
\end{align}

Define $p_{t}^{j}$ as the price of the intermediate product $y_{t}^{j}$ , then the representative firm minimizes the cost function $\int\limits_{0}^{1} p_{t}^{j} y_{t}^{j}dj$ of a particular combination of inputs observing the constraint \ref{Technology constraint}. Solving the minimization problem through Lagrangian, with $P_{t}$ as the Lagrange multiplier, yields:

\begin{align} \label{Optimal prices for firm}
Y_{t} &=  \left[ \int\limits_{0}^{1} \left( \frac{ p_{t}^{j}}{P_{t}} \right)^{\frac{-1}{\lambda_{p,t}}} Y_{t}^\frac{1}{1+\lambda_{p,t}} dj \right]^{1+\lambda_{p,t}} \nonumber \\
& \iff P_{t} = \left[ \int\limits_{0}^{1} \left( p_{t}^{j} \right)^{\frac{-1}{\lambda_{p,t}}} dj \right]^{-\lambda_{p,t}}
\end{align}

where $P_{t}$ can be interpreted as the price index of the final good sector, while $\lambda_{p,t}$ is a stochastic parameter representing the mark-up of the good market (or a cost-push shock), such that:

\begin{align}
\lambda_{p,t} = \lambda_{p} + \eta_{t}^{p}, \quad \eta_{t}^{p} \sim \mathcal{N} \left(0, \sigma^{2}_{\eta_{t}^{p}} \right)
\end{align}

\subsection{Intermediate product sector} \label{DSGE - Firms - Intermediate product sector}

Each firm $j$ of the intermediate good sector minimizes its labour and capital costs $W_{t}L_{j,t} + r_{t}^{k} \widetilde{K}_{j,t}$, observing its Cobb-Douglas production function:

\begin{align}
y_{t}^{j} = \varepsilon_{t}^{a} \widetilde{K}_{j,t}^{\alpha} L_{j,t}^{1-\alpha} - \Phi
\end{align}

where $\widetilde{K}_{j,t}$ is the effective utilized stock of capital given by $\widetilde{K}_{j,t} = z_{t}K_{j,t-1}$, $L_{j,t}$ is the index of the differently utilized labour force, while $\Phi$ are the fixed costs. Moreover, it is assumed that the aggregate productivity shock $\varepsilon_{t}^{a}$ follows a $AR(1)$ process such that:

\begin{align}
\varepsilon_{t}^{a} = \rho_{a} \varepsilon_{t-1}^{a} + \eta_{t}^{a}, \quad \eta_{t}^{a} \sim \mathcal{N}(0,1)
\end{align}

Each firm $j$ has a market power in the market of its good and hence it maximizes its expected profit $\pi_{t}^{j}$ with respect to $p_{t}^{j}$:

\begin{align} \label{Nominal profits}
\pi_{t}^{j} = \underbrace{p_{t}^{j} y_{t}^{j}}_\text{Revenues} - \underbrace{MC_{t} \left(y_{t}^{j} + \Phi \right)}_\text{Costs} = \left(p_{t}^{j} - MC_{t} \right) \underbrace{\left( \frac{p_{t}^{j}}{P_{t}} \right)^{\frac{-(1+\lambda_{p,t})}{\lambda_{p,t}}} Y_{t}}_{y_{t}^{j}} - MC_{t}\Phi
\end{align}

where marginal costs takes the form:

\begin{align}
MC_{t} = \frac{1}{\varepsilon_{t}^{a}} W_{t}^{1-\alpha} \left(r_{t}^{k} \right)^{\alpha} \left( \alpha^{-\alpha} (1-\alpha)^{-(1-\alpha)}\right)
\end{align}

The profits of intermediate goods firms are assumed to be returned as dividends $D_{t}$.

In order to perform the maximization task, it uses a discount factor ($\beta^{k} \rho_{t +k}$) that is based on the fact that firms belong to households. From the Euler-equation of consumption, it holds:


\begin{align} \label{Euler for firm}
\beta^{k} \rho_{t +k} = \beta^{k} \frac{\lambda_{t+k}}{\lambda_{t}P_{t+k}}
\end{align} 

Finally, the determination of prices also follows a Calvo-rule in order to model the nominal price rigidity. Each firm $j$ is allowed in period $t$ to peg its nominal price with probability $\varsigma_{p}$. Since firms are defined over a continuum between 0 and 1, prices that are updated in each period are $1-\varsigma_{p}$ and prices that are not updated amount to $\varsigma_{p}$. Prices of firms that was unable to re-set their prices are partially indexed by the past inflation rate $\Pi_{t-1}$:

\begin{align}
p_{t}^{j} = \left( \Pi_{t-1} \right)^{\gamma_{p}}p_{t-1}^{j} = \left( \frac{P_{t-1}}{P_{t-2}} \right)^{\gamma_{p}}p_{t-1}^{j}
\end{align}

where $\gamma_{p}$ is the degree of price indexation, such that $\gamma_{p} = 0$ entails no indexation ($p_{t}^{j} = p_{t-1}^{j}$), while $\gamma_{p} = 1$ suggests a complete indexation ($p_{t}^{j} = \Pi_{t-1} p_{t-1}^{j}$).

After denoting by $\widetilde{p}_{t}$ nominal price of the firm that is able in period t to reoptimize, it follows:

\begin{align}
p_{t}^{j}  = \begin{cases}
\widetilde{p}_{t} & \text{with probability} \enspace 1-\varsigma_{p} \\
\left( \Pi_{t-1} \right)^{\gamma_{p}}p_{t-1}^{j} & \text{with probability} \enspace \varsigma_{p} 
\end{cases}
\end{align}

The equation of motion for the aggregated price index $P_{t}$ can be obtained through \ref{Optimal prices for firm}:

\begin{align}
\left( P_{t} \right)^{\frac{1}{-\lambda_{p,t}}} &= \varsigma_{p} \int\limits_{0}^{1} \left( P_{t-1}\left(\frac{P_{t-1}}{P_{t-2}}\right)^{\gamma_{p}} \right)^{\frac{-1}{\lambda_{p,t}}} dj + (1-\varsigma_{p}) \int\limits_{0}^{1} \left(\widetilde{p}_{t} \right)^{\frac{-1}{\lambda_{p,t}}} dj = \nonumber \\
& =  \varsigma_{p} \left[ P_{t-1}\left(\frac{P_{t-1}}{P_{t-2}}\right)^{\gamma_{p}} \right]^{\frac{-1}{\lambda_{p,t}}} + (1-\varsigma_{p}) \left(\widetilde{p}_{t} \right)^{\frac{-1}{\lambda_{p,t}}}
\end{align}

The optimal price $\widetilde{p}_{t}$ set in t has a probability of $\varsigma_{p}^{i}$ not to be modified until period $t+i$. Furthermore, through the indexation procedure it holds that:

\begin{align} \label{Firm constraint}
p_{t}^{j} &= \widetilde{p}_{t} \nonumber \\
p_{t+1}^{j} &= \left( \frac{P_{t}}{P_{t-1}} \right)^{\gamma_{p}} p_{t}^{j} = \left( \frac{P_{t}}{P_{t-1}} \right)^{\gamma_{p}} \widetilde{p}_{t}  \nonumber \\
p_{t+2}^{j} &= \left( \frac{P_{t+1}}{P_{t}} \right)^{\gamma_{p}}  p_{t+1}^{j} = \left( \frac{P_{t+1}}{P_{t}} \right)^{\gamma_{p}} \left( \frac{P_{t}}{P_{t-1}} \right)^{\gamma_{p}}  \widetilde{p}_{t} = \left( \frac{P_{t+1}}{P_{t-1}} \right)^{\gamma_{p}} \widetilde{p}_{t} \nonumber \\ \vdots \nonumber \\
p_{t+i}^{j} &= \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}} \widetilde{p}_{t} \nonumber \\
\end{align}

Firms that are not allowed to change their prices in period t, maximize their profit function \ref{Nominal profits} under the constraint \ref{Firm constraint}, which yields:

\begin{align}
&\mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \frac{\lambda_{t+i}}{\lambda_{t}} y_{t+i}^{j} \left\{ \frac{\widetilde{p}_{t}}{P_{t}} \left[ \frac{ \left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}}{\frac{P_{t+i}}{P_{t}}} \right] - (1+\lambda_{p,t+i})\frac{MC_{t+i}}{P_{t+i}} \right\} = 0
\end{align}

Assuming perfect flexibility of prices $(\varsigma_{p} = 0)$, the above-mentioned equation becomes:

\begin{align}
\widetilde{p}_{t} = \left(1 + \lambda_{p, t} \right) \cdot MC_{t}
\end{align}

The optimal price is set so that firms impose a premium over the average marginal costs.

\section{The log-linearised DSGE model} \label{The log-linerised DSGE model}

In the following section, the log-linearised version of the DSGE model adopted in this paper is described. The hat symbol above the variable will denote the log deviations from its steady state. The dynamics of aggregate consumption is seized by:

\begin{align}
R_{t} + \mathbb{E}_{t} \hat{\varepsilon}^{B}_{t+1} - \frac{\sigma_{c}}{1-h} \left( \mathbb{E}_{t}\hat{C}_{t+1} - h \hat{C}_{t} \right) = \hat{\varepsilon}^{B}_{t+1} - \frac{\sigma_{c}}{1-h} \left( \hat{C}_{t} - h \hat{C}_{t-1} \right) + \mathbb{E}_{t} \hat{\Pi}_{t+1}
\end{align}

For h = 0 one obtains the usual Euler equation, nevertheless in the presence of the external habit, present consumption depends on both the expected consumption of the next period and on consumption of the last period. The elasticity of consumption with respect to interest rates depends both on the intertemporal elasticity of substitution $\sigma_{c}$ and on the degree of the habit $h$. A higher $h$ reduces the effect of a variation of interest rate on consumption holding $\sigma_{c}$ constant.

In addition, through rearranging terms, one obtains an explicit expression for $\hat{C}_{t}$:

\begin{align}
\hat{C}_{t} = \frac{h}{1+h}\hat{C}_{t-1}+ \frac{1}{1+h}\mathbb{E}_{t}\hat{C}_{t+1} - \frac{1-h}{(1+h)\sigma_{c}} \left( \hat{R}_{t} - \mathbb{E}_{t} \hat{\Pi}_{t+1} \right) + \frac{1-h}{(1+h)\sigma_{c}} \left( \hat{\varepsilon}_{t}^{B} - \mathbb{E}_{t}\hat{\varepsilon}_{t+1}^{B} \right)
\end{align}

Furthermore, the linearization of the investment equation yields:

\begin{align}
\hat{I}_{t} = \frac{1}{1+\beta} \hat{I}_{t-1} + \frac{\beta}{1+\beta} \mathbb{E}_{t} \hat{I}_{i+1} + \dfrac{1}{S''(1)(1+\beta)}\hat{Q}_{t} + \frac{\beta \mathbb{E}_{t} \hat{\varepsilon}_{t+1}^{I} - \hat{\varepsilon}_{t}^{I}}{1+\beta}
\end{align}

The dependency of the optimal investment of past values introduce the dynamic in this equation. In addition, a positive shock of the adjustment costs function reduces investment and hence it can be considered as a negative investment shock.

Due to the steady state condition $\beta = \frac{1}{1-\tau +r^{k}}$, the linearization of the Q-equation yields:

\begin{align}
\hat{Q}_{t} = - \left( \hat{R}_{t} - \mathbb{E}\hat{\pi}_{t+1} \right) + \frac{1-\tau}{1-\tau +r^{k}} \mathbb{E}_{t} \hat{Q}_{t+1} + \frac{r^{k}}{1-\tau +r^{k}} \mathbb{E}_{t}\hat{r}^{k}_{t+1} + \eta_{t}^{Q}
\end{align}

An equity premium shock $\eta_{t}^{Q}$, normally and independently distributed, is added in order to take into account the effect of changes of the external refinancing premium over the the capital stock value.

The linearization of the equation of the capital accumulation yields:

\begin{align}
\hat{K}_{t} = (1-\tau)\hat{K}_{t-1} + \tau\hat{I}_{t-1}
\end{align}

Through the partial indexation, it follows that the linearized prices equation depends not only on the present marginal costs and on the expectations of the future inflation but also on the past inflation rate.

\begin{align}
\hat{\pi}_{t} = \frac{\beta}{1+\beta \gamma_{p}} \mathbb{E}_{t} \hat{\pi}_{t+1} + \frac{\gamma_{p}}{1+\beta \gamma_{p}} \hat{\pi}_{t-1} + \frac{\left( 1 - \beta \varsigma_{p} \right) \left( 1 - \varsigma_{p} \right)}{\left(1+\beta \gamma_{p} \right) \varsigma_{p}} \left( \underbrace{ \alpha \hat{r}^{k}_{t} + (1-\alpha)\hat{w}_{t} - \hat{\varepsilon}_{t}^{a} + \eta_{t}^{p} }_{\hat{mc}_{t}} \right)
\end{align}

For $\gamma_{p} = 0$ one obtains the usual forward-looking Phillips curve.

Similarly one obtains the linearized equation of motion for the real wage:

\begin{align}
\hat{w}_{t} &= \frac{\beta}{1+\beta} \mathbb{E}_{t} \hat{w}_{t+1} + \frac{1}{1+\beta} \hat{w}_{t-1} + \frac{\beta}{1+\beta} \mathbb{E}_{t} \hat{\pi}_{t+1} - \frac{1+ \beta \gamma_{w}}{1+\beta} \hat{\pi}_{t} + \nonumber \\
& \quad + \frac{\gamma_{w}}{1+\beta} \hat{\pi}_{t-1} - \frac{\left( 1 - \beta \varsigma_{w} \right) \left( 1 - \varsigma_{w} \right)}{\left(1+\beta \right) \left( 1 + \frac{\left(1+\lambda_{w}\right)\sigma_{l}}{\lambda_{w}}  \right) \varsigma_{p}} \left[
\hat{w}_{t} - \sigma_{l} \hat{L}_{t} - \frac{\sigma_{c}}{1-h} \left( \hat{C}_{t} - h \hat{C}_{t-1} \right) - \hat{\varepsilon}_{t}^{L} - \hat{\eta}_{t}^{w} \right]
\end{align}

The real wage is a function of expected and past real wages and the expected, current and past inflation rate if $\gamma_{w}>0$. There is a negative effect of the deviation of the actual real wage from one that would abound in a flexible labour market.

The linearized demand of labor is:

\begin{align}
\hat{L}_{t} = - \hat{w}_{t} + \left( 1 + \frac{\Psi'(1)}{\Psi''(1)} \right) \hat{r}^{k}_{t} + \hat{K}_{t-1}
\end{align}

or equivalently:

\begin{align}
\hat{L}_{t} = - \hat{w}_{t} + \left( 1 + \Psi \right) \hat{r}^{k}_{t} + \hat{K}_{t-1}
\end{align}

where $\Psi = \frac{\Psi'(1)}{\Psi''(1)}$ is the inverse of the elasticity of the capital utilization cost function. In other words, given a capital stock, the demand of labor negatively depends on real wages.

Furthermore, the good market is in equilibrium if the production is equal to the public expense $G_{t}$ , the household demand of consumption and investment plus related costs:

\begin{align}
Y_{t} = C_{t} + G_{t} + I_{t} + \Psi(z_{t})K_{t-1}
\end{align}

where government consumption $G_{t}$ is financed by the lump sum taxation:

\begin{align}
G_{t} = T_{t}
\end{align}

In the steady state deviations:

\begin{align}
\hat{Y}_{t} &= \left( 1 - \tau k_{y} - g_{y} \right) \hat{C}_{t} + \tau k_{y} \hat{I}_{t} + \varepsilon_{t}^{G} 
\end{align}

where $k_{y}$ and $g_{y}$ denote the steady state proportions of capital and public spending on output. On the other hand, from the production function:

\begin{align}
\hat{Y}_{t} & = \vartheta \hat{\varepsilon}_{t}^{a} + \vartheta \alpha \hat{K}_{t-1} + \vartheta \alpha \frac{\Psi'(1)}{\Psi''(1)} \hat{r}_{t}^{k} + \vartheta(1-\alpha)\hat{L}_{t}
\end{align}

where $1+\vartheta$ is the proportion of fix costs of the production. Moreover, it is assumed that the shock of the public spending $\varepsilon_{t}^{G}$ follows a $AR(1)$ process:

\begin{align}
\varepsilon_{t}^{G} = \rho_{G} \varepsilon_{t-1}^{G} + \eta_{t}^{G}, \quad \eta_{t}^{G} \sim \mathcal{N}(0,1)
\end{align}

Analogously, the capital market is in equilibrium if the demand (of firms of the intermediate sector) of capital goods equals the supply of households:

\begin{align}
\int\limits_{0}^{1} K_{j,t-1} \, dj = K_{t-1}
\end{align} 

while the labour market is in equilibrium if the demand of labour of firms equals the supply of households at wages set by households:

\begin{align}
L_{t} &= \left[ \int\limits_{0}^{1} \left( l_{t}^{\tau} \right)^{\frac{1}{1+\lambda_{w,t}}} d\tau \right]^{1+\lambda_{w,t}}
\end{align}

Finally, the monetary policy follows a generalized Taylor Rule:

\begin{align}
\hat{R}_{t} =& \rho \hat{R}_{t-1} + (1-\rho) \left[ \bar{\pi}_{t} + r_{\pi} \left( \hat{\pi}_{t-1} - \bar{\pi}_{t} \right) + r_{y} \hat{Y}_{t}^{\text{gap}} \right] + \nonumber \\
& + r_{\Delta_{\pi}} \left( \hat{\pi}_{t} - \hat{\pi}_{t-1} \right) + r_{\Delta_{Y}} \left( \hat{Y}_{t}^{\text{gap}} - \hat{Y}_{t-1}^{\text{gap}} \right) + \eta_{t}^{R}
\end{align}

The Taylor rule is obtained by minimizing the central bank loss function subject to the Phillips curve and the IS curve. The central bank loss function is generally set to be equal to a weighted sum of the squared difference between the actual and the target inflation and the squared output gap. The output gap $\hat{Y}_{t}^{\text{gap}}$ is the difference between the actual and the potential output. The potential output is the production that theoretically the economy performs under perfect flexibility of prices and wages.

The central bank reacts to deviations of both the lagged inflation with respect to the target inflation (normalized to 0) and of the lagged output gap. The monetary policy is subject to two different kinds of shock: a shock of the target inflation process $ \bar{\pi}_{t} = \rho_{\pi} \bar{\pi}_{t-1} + \eta_{t}^{\pi}$ and shock
of the interest rate $\eta_{t}^{R}$. The central smooths the interest rates by setting the present rate as a weighted average of the past interest rate and the optimal rate as a function of the above quantities.

To sum up, in order to calculate the results, two systems have to be created: one is the flexible scheme without price stickiness, wage stickiness or three cost-push shocks, on contrary the other one should be viewed as the sticky system where prices and wages follow a Calvo mechanisms. Then the potential output obtained from the flexible system is implemented to compute the output gap in the Taylor rule. In each system, there are eight endogenous variables (capital, consumption, investment, inflation, wages, output, interest rate and real capital stock) and two state variables (labor and return on capital).

\chapter{Solution}

Since \citet{blanchard1980solution} a significant amount of substitutable approaches for solving linear rational expectations models have materialized, not to mention the methods of \citet{uhlig1998toolkit}, \citet{klein2000using} or \citet{sims2002solving}. Economists adopt then their output for simulating and estimating models, calculating impulse response functions and asymptotic covariance, solving infinite horizon linear quadratic control problems or finally constructing terminal constraints for nonlinear models. Despite the fact that for models satisfying the \citet{blanchard1980solution} conditions, the algorithms provide equivalent solutions, three algorithms will be presented in depth: the approach of \citet{klein2000using} for the illustrational purposes, the \citet{uhlig1998toolkit} toolkit for the actual implementation in Python and the Linear Time Iteration algorithm, whose differentiability will be substantial for the Automatic Differentiation application.

\section{\citet{klein2000using} approach}

DSGE model consists of state variables ($x_{t}$), control variables ($y_{t}$), transition equations for structural shocks and measurement errors ($\varepsilon_{t}$) and a vector of deep parameters $\theta$, which can be injected into a nonlinear first-order system of expectational difference equations:

\begin{align}
\mathbb{E}_{t}f\left(x_{t+1},y_{t+1},x_{t},y_{t} | \, \theta \right) = 0
\end{align}

Solution is then characterized by transition equations for state and control variables, so-called \textit{policy-functions}, that solve (at least approximately) the aforementioned system of equations:

\begin{align}
\left\{
\begin{array}{cl}
x_{t+1} &= \widetilde{h} \left( x_{t} | \, \theta \right) + \eta_{x} \left( \theta \right) \varepsilon_{t+1} \\
y_{t} &= \widetilde{g} \left( x_{t} | \, \theta \right)
\end{array}
\right.
\end{align}

Unfortunately, in the majority of cases the analytical derivation of $g$ and $h$ is extremely difficult or even impossible. One therefore distinguishes between linear (such as in \citet{klein2000using} or \citet{sims2002solving}) and non-linear methods (for instance \citet{schmitt2004solving} or \citet{gomme2011second}) of local approximation of the policy functions in a neighborhood of a precise point (mostly steady-state). The fundamental idea is to introduce the perturbation parameter $\sigma$ that captures the stochastic nature of the model. The general perturbation framework (in the spirit of \citet{schmitt2004solving}) is as follows:

\begin{align}
\left\{
\begin{array}{cl}
0 &= \mathbb{E}_{t}f\left(x_{t+1},y_{t+1},x_{t},y_{t} | \, \theta \right) \\
x_{t+1} &= h \left( x_{t}, \sigma | \, \theta \right) + \sigma \eta_{x} \varepsilon_{t+1} \\
y_{t} &= g \left( x_{t}, \sigma | \, \theta \right)
\end{array}
\right.
\end{align}

with $\mathbb{E}(\varepsilon_{t}) = 0$ and $\mathbb{E}(\varepsilon_{t} \varepsilon_{t}^{'} ) = \Sigma_{\varepsilon}$. The non-stochastic steady state is then defined as:

\begin{align}
\left\{
\begin{array}{cl}
0 &= f \left(\bar{x}, \bar{y}, \bar{x}, \bar{y}| \, \theta \right) \\
\bar{y} &= h \left( \bar{x}, 0 | \, \theta \right) \\
\bar{y} &= g \left( \bar{x}, 0 | \, \theta \right)
\end{array}
\right.
\end{align}

where exogenous shocks and measurement errors are stacked into $\varepsilon_{t}$ for convenience.

As far as the first-order approximations of $g$ and $h$ around the point $(x_{t}, \sigma) = (\bar{x}, 0)$ are concerned:

\begin{align}
\left\{
\begin{array}{cl}
g(x_{t}, \sigma) = g(\bar{x}, 0) + g_{x}(\bar{x}, 0)(x_{t} - \bar{x}) + g_{\sigma}(\bar{x}, 0)(\sigma - 0) \\
h(x_{t}, \sigma) = h(\bar{x}, 0) + h_{x}(\bar{x}, 0)(x_{t} - \bar{x}) + h_{\sigma}(\bar{x}, 0)(\sigma - 0)
\end{array}
\right.
\end{align}

Substituting into the model:

\begin{align}
F(x_{t}, \sigma) = \mathbb{E}_{t} f \left( \underbrace{h(x_{t}, \sigma) + \sigma \varepsilon_{t+1}}_{x_{t+1}}, \underbrace{g \left( \overbrace{h\left(x_{t}, \sigma \right) + \sigma \varepsilon_{t+1}}^{x_{t+1}}, \sigma \right)}_{y_{t+1}}, x_{t}, \underbrace{g(x_{t}, \sigma)}_{y_{t}} \right)
\end{align}

After noticing that $f$ and its derivatives are 0 when assessed at the non-stochastic steady state $\left(\bar{x}, 0\right)$, taking derivative of $F$ with respect to $\sigma$ and evaluating at the non-stochastic steady state yields:

\begin{eqnarray}
&F_\sigma(\overline{x},0) = E_t f_1[h_\sigma + \varepsilon_{t+1}] + E_t f_2 [g_x(h_\sigma+ \varepsilon_{t+1})+g_\sigma] + f_3\cdot 0 + f_4 g_\sigma = 0 \nonumber \\
 &  \iff \begin{bmatrix} \underset{(n \times n_x)}{f_1 + f_2 g_x} & \underset{(n\times n_y)}{f_2 +f_4}\end{bmatrix} \begin{bmatrix} \underset{(n_x \times 1)}{h_\sigma} \\ \underset{(n_y \times 1)}{g_\sigma} \end{bmatrix} = \underset{(n \times 1)}{0}
  \end{eqnarray}

with $f_1= \frac{\partial f (\bar{x}, 0)}{\partial x_{t+1}}, f_2=  \frac{\partial f (\bar{x}, 0)}{\partial y_{t+1}},  f_3=\frac{\partial f (\bar{x}, 0)}{\partial x_{t},}  f_4=\frac{\partial f (\bar{x}, 0)}{\partial y_{t}}$. \\

There are $n$ equations in $n$ unknowns in a linear and homogenous system, hence in the case of unique solution, it must be:

\begin{align}
\left\{
\begin{array}{cl}
h_\sigma &= \underset{(n_x\times 1)}{0} \\
g_\sigma &= \underset{(n_y \times 1)}{0} 
\end{array}
\right.
\end{align}

Furthermore, taking derivative of $F$ with respect to $x_t$ and evaluating at the non-stochastic steady state yields:

\begin{eqnarray}
&    F_x (\overline{x},0) = f_1 h_x + f_2 g_x h_x + f_3 + f_4 g_x = 0 \nonumber \\
& \iff \underbrace{- \begin{bmatrix} \underset{(n\times n_x)}{f_1} & \underset{(n\times n_y)}{f_2} \end{bmatrix}}_{:=A} \begin{bmatrix} \underset{(n_x\times n_x)}{h_x} \\ \underset{(n_y\times n_x)}{g_x} \underset{(n_x\times n_x)}{h_x} \end{bmatrix}  = \underbrace{\begin{bmatrix} \underset{(n\times n_x)}{f_3} & \underset{(n\times n_y)}{f_4}\end{bmatrix}}_{:=B} \begin{bmatrix} \underset{(n_x\times n_x)}{I} \\\underset{(n_y\times n_x)}{g_x} \end{bmatrix}
\end{eqnarray}
  
Therefore there are $n\times n_x$ equations for $n\times n_x$ unknown elements of both $h_x$ and $g_x$. One can additionally multiply both sided by the deviation from the steady state $\widehat{x}_t := (x_t-\overline{x})$, what yields:

 \begin{align}
 A \begin{bmatrix} h_x \widehat{x}_t \\ g_x h_x \widehat{x}_t \end{bmatrix} = B \begin{bmatrix} \widehat{x}_t \\ g_x \widehat{x}_t \end{bmatrix}
  \end{align}

It is useful to discern that the coefficient matrices are tantamount to the first order approximation:

\begin{eqnarray}
A \mathbb{E}_t\begin{bmatrix} \widehat{x}_{t+1} \\ \widehat{y}_{t+1} \end{bmatrix} = B\begin{bmatrix} \widehat{x}_t\\ \widehat{y}_t \end{bmatrix} + \begin{bmatrix} \sigma \eta_x \varepsilon_{t+1} \\ 0\end{bmatrix}
\end{eqnarray}

If $A$ and $B$ are known matrices, then undoubtedly inverting $A$ yields solution for $h_x$ and $g_x$. Moreover, $A$ is generally singular and non-invertible. \cite{klein2000using} approach to approximate the first order solution employing the generalized Schur decomposition is therefore implemented, in order to uncouple system into a (block) triangular system of equations and solve system recursively.
 
\subsection{Matrix theory}

\begin{mydef}
Let $A$ and $B$ be two square ($n\times n$) matrices. The set of all matrices of the form $A-\lambda B$ with $\lambda \in \mathbb{C}$ is forenamed to be a \textbf{pencil}. The eigenvalues of the pencil are elements of the set $\lambda(A,B)$ characterized by:
  \begin{align*}
    \lambda(A,B) =  \left\{z \in \mathbb{C}:det(A-zB)=0\right\}
  \end{align*}
\end{mydef}

\begin{mydef}
Let $A$ and $B$ be two square ($n\times n$) matrices. Then $\lambda \in \lambda(A,B)$ is forenamed a \textbf{generalized eigenvalue} if there exists a nonzero vector $q\in \mathbb{C}^n$ such that $$A q = \lambda B q$$
\end{mydef}

It is advantageous to detect that if $B=I$, then the above-mentioned intricacy simplifies to the ordinary eigenvalue problem: $A q = \lambda q$. Moreover, the $n$ eigenvalues of A can be ordered (generally in more than one way) to form an $n\times n$ diagonal matrix $\Lambda$ and a corresponding matrix of nonzero columns $Q$ that satisfies the eigenvalue equation:

\begin{align}
AQ = Q\Lambda
\end{align}

\begin{theorem}[Spectral decomposition]
Assume that $A$ is a square $(n\times n)$ matrix with $n$ linearly independent columns. Then $A$ can be factorized as:
  \begin{align*}
  AQ = Q \Lambda \Leftrightarrow A = Q \Lambda Q^{-1}
  \end{align*} 
where $\Lambda$ is a diagonal matrix such that $\lambda_i =\Lambda_{ii}$ is the eigenvalue of $A$ related to the eigenvector $q_i$ gathered in column $i$ of the squared $(n \times n)$ matrix $Q$.
\end{theorem}

The proofs for the following section can be found for instance in \citet{roman2008graduate} or \citet{brown1988second}.  

\begin{theorem}[Generalized (complex) Schur decomposition or QZ decomposition]
Let $A$ and $B$ be square ($n\times n$) matrices. Then there exist matrices $Q$, $Z$, $T$ and $S$ such that
\begin{align*}
  Q^* A Z &= S \Leftrightarrow A = Q S Z^*\\
  Q^* B Z &= T \Leftrightarrow B = Q T Z^*
\end{align*}
\begin{enumerate}
  \item $Q$ and $Z$ are unitary, i.e. $Q^*Q=QQ^*=I_n$ and $Z^*Z=ZZ^*=I_n$.
  \item $S$ and $T$ are upper triangular.
 \item Pairs $(s_{ii},t_{ii})$ can be arranged in any desired order.
  \item If for some $i$, $t_{ii}$ and $s_{ii}$ are both zero, then $\lambda(A,B)=\mathbb{C}$. Otherwise:
$\lambda(A,B) = \left\{\frac{s_{ii}}{t_{ii}}:t_{ii} \neq 0 \right\}$
\end{enumerate}
\end{theorem}

The interest in the following paper will be limited to the case $\lambda(A,B)\neq \mathbb{C}$ and unit roots will not be allowed, that is $t_{ii}$ and $s_{ii}$ are not both zero, as well as $|t_{ii}|\neq |s_{ii}|$.

\subsection{Schur decomposition application}

Returning to the problem of finding $g_x$ and $h_x$:

\begin{eqnarray}
    A E_t \begin{bmatrix} \widehat{x}_{t+1} \\ \widehat{y}_{t+1} \end{bmatrix} = B\begin{bmatrix} \widehat{x}_t\\ \widehat{y}_t \end{bmatrix}
  \end{eqnarray}

The Schur decompositions of $A$ and $B$ are given by:

\begin{align}
\left\{
\begin{array}{cl}
Q^* A = S Z^* \\
Q^* B = T Z^*
\end{array}
\right.
\end{align}

where the following order is chosen: the stable generalized eigenvalues $(|s_{ii}|>|t_{ii}|)$ come first. 

Multiplying both sides by $Q^*$ and using:

\begin{align}
\begin{bmatrix} \underset{n_x \times 1}{s_t} \\ \underset{n_y\times1}{u_t} \end{bmatrix} : = Z^* \begin{bmatrix} \underset{n_x \times 1}{\widehat{x}_t} \\ \underset{n_y \times 1}{\widehat{y}_t} \end{bmatrix}\end{align}

one gets:

\begin{align}
  S  \begin{bmatrix} E_t s_{t+1}\\ E_t u_{t+1} \end{bmatrix} = T \begin{bmatrix} s_t \\ u_t \end{bmatrix}
\end{align}

where $S$ and $T$ are upper triangular, such that:

\begin{align}
\begin{bmatrix} \underset{n_x \times n_x}{S_{11}} & \underset{n_x \times n_y}{S_{12}} \\ \underset{n_y \times n_x}{0} & \underset{n_y \times n_y}{S_{22}}\end{bmatrix} \begin{bmatrix} E_t s_{t+1}\\ E_t u_{t+1} \end{bmatrix} = \begin{bmatrix} \underset{n_x \times n_x}{T_{11}} & \underset{n_x \times n_y}{T_{12}} \\ \underset{n_y \times n_x}{0} & \underset{n_y \times n_y}{T_{22}}\end{bmatrix} \begin{bmatrix} s_t \\ u_t \end{bmatrix}
\end{align}

Solving the lower block first:

\begin{align}
  S_{22} E_t[u_{t+1}] = T_{22} u_t
\end{align}

Note that due to the ordering $|s_{ii}|<|t_{ii}|$ (or $|s_{ii}|\leq|t_{ii}|$ ). Using backward-substitution, it can be proved, that each solution with bounded mean and variance must satisfy $u_t=0$ for all $t$ (unless $\Sigma=0$), otherwise the solution explodes. In other words, as many state variables as there are generalized eigenvalues with $|s_{ii}|>|t_{ii}|$ are needed. This property has a name:

\begin{mydef}[\citet{blanchard1980solution} conditions]
The number of generalized eigenvalues, that are in absolute terms greater than 1, must be equal to the number of state variables, in order to get a stable solution (saddle-path).
\end{mydef}

Furthermore, given the fact that $u_t=0$, both $x_t$ and $y_t$ from the definition of $s_t$ and $u_t$ become:

\begin{align}
  \begin{bmatrix} x_t \\ y_t \end{bmatrix} = \begin{bmatrix} Z_{11} & Z_{12}\\ Z_{21} & Z_{22} \end{bmatrix} \begin{bmatrix} s_t \\ u_t \end{bmatrix}
  \Rightarrow
  \begin{cases}
    x_t = Z_{11} s_t\\
    y_t = Z_{21} s_t
  \end{cases}
\end{align}

If $Z_{11}$ is nondegenerate, then:

\begin{align}
  y_t = \underbrace{Z_{21} Z_{11}^{-1}}_{=g_x} x_t
\end{align}

Thus, in order to compute $g_x$ a nonsingular $Z_{11}$ is needed. Assuming  that $S_{11}$ is invertible, the first block yields:

\begin{align}
   E_t[s_{t+1}] &= S_{11}^{-1} T_{11} s_t
\end{align}

Plugging in $s_t = Z_{11}^{-1} x_t$ one gets:

\begin{align}
   E_t[ x_{t+1}] = \underbrace{Z_{11} S_{11}^{-1} T_{11} Z_{11}^{-1}}_{=h_x} x_t
\end{align}

Thus, in order to compute $h_x$, nonsingular $S_{11}$ and $Z_{11}$ are required. However, $S_{11}$ has full rank by construction. The \citet{klein2000using} algorithm can be abridged in the following manner: \\

\begin{algorithm}[H]
\caption{\citet{klein2000using} algorithm}
 \KwData{Nonlinear first-order system of expectational difference equations\;}
 \KwResult{DSGE solution\;}
Calculate the generalized Schur decomposition of: \\
\qquad \begin{align*} \begin{cases}
A=-\begin{bmatrix} f_1 & f_2\end{bmatrix} \\
B=\begin{bmatrix} f_3 & f_4\end{bmatrix}
\end{cases} \end{align*} \\
Reorder generalized eigenvalues such that $|s_{ii}|>|t_{ii}|$ in the upper left\;
  \eIf{\citet{blanchard1980solution} condition fulffiled}{
   \eIf{$Z_{11}$ invertible}{
Compute $h_x=Z_{11} S_{11}^{-1} T_{11} Z_{11}^{-1}$ and $g_x=Z_{21} Z_{11}^{-1}$\;
}{
Infeasible solution\;}
 }{
   Infeasible solution\;
  }
\end{algorithm}

\bigbreak

Note that the squareness of $Z_{11}$ is referred to in the literature as the \citet{blanchard1980solution} order condition, while the non-singularity of $Z_{11}$ as the \citet{blanchard1980solution} rank condition.

\section{\citet{uhlig1998toolkit} toolkit}

The \citet{uhlig1998toolkit} algorithm likewise employs generalized eigenvalue calculations to glean a solution for the matrix polynomial equation, based on the method of undetermined coefficients. The method is applied to systems written as:

\begin{align}
0 &= Ax' + Bx + Cy + Dz \nonumber \\
0 &=\mathbb{E} \left[Fx'' + Gx' + Hx + Jy' + Ky + Lz' + Mz \right] \nonumber \\
z' &= Nz + \varepsilon'
\end{align}

where $x$ are the endogenous state variables, $y$ the endogenous control (also called \textit{jump}) variables, while $z$ denotes the exogenous variables. Moreover, the apostrophe stands for the leading operator.

Assuming that the decision rules can be expressed as:

\begin{align}
x' &= Px + Qz \nonumber \\
y &= Rx + Sz
\end{align}

one can substitute the decision rules into the system:

\begin{align}
0 &= (AP+B+CR)x+(AQ+CS+D)z \nonumber \\
0 &= (GP+H+JRP+KR)x+(GQ+JRQ+JSN+KS+M)z
\end{align}

Since these equations must hold for any $x$ and $z$:

\begin{align}
0 &= AP+B+CR  \label{Uhlig1} \\
0 &= AQ+CS+D  \label{Uhlig2} \\
0 &= GP+H+JRP+KR  \label{Uhlig3} \\
0 &= GQ+JRQ+JSN+KS+M \label{Uhlig4}
\end{align}

Solving \ref{Uhlig1} for $R$:

\begin{align}
R = -C^{-1} \left(AP + B \right)
\end{align}

Substituting it for R into equation \ref{Uhlig3}:

\begin{align}
-JC^{-1}AP^{2} - \left(JC^{-1}B + KC^{-1}A - G \right)P - \left(KC^{-1}B - H \right) = 0
\end{align} 

The above-mentioned quadratic matrix equation in $P$ can be then solved by generalized Schur decomposition. Next, solving for $S$ as the function of $Q$ with \ref{Uhlig2}:

\begin{align}
S = -C^{-1} \left(AQ + D \right)
\end{align}

Substituting further in \ref{Uhlig4}

\begin{align}
GQ+JRQ-JC^{-1} (AQ+D)N - KC^{-1} (AQ+D)+M &= 0 \nonumber \\
\Rightarrow \left(G + JR - KC^{-1}A \right)Q - JC^{-1}DN - KC^{-1}D + M &= 0
\end{align}

Matrix $Q$ is the only unknown, but it is not trivial, as $Q$ is sandwiched. Nevertheless, one can use the Kronecker product $\otimes$ such that $vec \left(XYZ\right) = \left(Z^{T} \otimes X \right) vec(Y)$, applied to \ref{Uhlig4}:

\begin{align}
0 &= I \otimes \left(G + JR - KC^{-1}A \right) vec(Q) - N^{T} \otimes \left(JC^{-1}A\right) vec(Q) - vec\left(JC^{-1}DN - KC^{-1}D + M \right) \nonumber \\
& \Rightarrow vec(Q) = \left[ I \otimes \left(G + JR - KC^{-1}A \right) - N^{T} \otimes \left(JC^{-1}A \right)\right]^{-1} vec\left(JC^{-1}DN - KC^{-1}D + M \right)
\end{align}

It has to be underlined that for the \citet{uhlig1998toolkit} procedure, one must choose \textit{jump} variables to maintain the full rank of $C$ matrix, namely augment the system with a \textit{dummy variable} and respective equations.

Solution of \citet{uhlig1998toolkit} toolkit is a state space system: a transition equation supplemented with a measurement equation. Moreover, it can be viewed as a restricted VAR(1):

\begin{align}
Y' = B_{1}Y+ B_{0}z
\end{align}

where:

\begin{align}
\left\{
\begin{array}{cl}
B_{0} &= \begin{bmatrix}
    SS \\
    QQ
\end{bmatrix} \\
B1 &=  \begin{bmatrix}
    0 & RR \\
    0 & PP
\end{bmatrix} \\
Y' &= \begin{bmatrix}
    y' \\
    x'
\end{bmatrix}
\end{array}
\right.
\end{align}

Finally, it has to be emphasised that the \citet{uhlig1998toolkit} and \citet{klein2000using} solutions are equivalent (\citet{bonaldi2010identification}).

\section{Linear Time Iteration procedure} \label{LTI}

\citet{uhlig1998toolkit} procedure provided the foundations for the techniques to solve a  \citet{smets2003estimated} model. Nevertheless, due to the implementation of non-differentiable QZ-algorithm, its usefulness for the machine learning frameworks exploiting the benefits of automatic differentiation is questionable. Therefore, the Linear Time Iteration Procedure of \cite{rendahl2017linear} is proposed as an alternative, yielding identical results to the \citet{uhlig1998toolkit} approach.

The model of interest is once again given by:

\begin{align}
AX_{t-1} + BX_{t} + CX_{t+1} + \mathbf{u}_{t} = 0
\end{align}

where $X_{t}$ is an $n \times 1$ vector containing endogenous and exogenous variables, $\mathbf{u}_{t}$ is an $n \times 1$ vector of mean-zero disturbances, and $A$, $B$ and $C$ are conformable matrices.

Similarly, one assumes the recursive solutions in form:

\begin{align}
X_{t} = FX_{t-1} + G\mathbf{u}_{t}
\end{align}

Inserting into the preceding equation:

\begin{align}
AX_{t-1} + BX_{t} + CFX_{t} + \mathbf{u}_{t} = 0
\end{align}

Thus, the matrix $Q$ is trivially defined by:

\begin{align}
G = -\left( B + CF \right)^{-1}
\end{align}

Moreover, $F$ must be the solution to the quadratic matrix equation:

\begin{align}
A + BF + CF^{2} = 0
\end{align}

Undoubtedly, if $F$ is known, finding $Q$ is a trivial operation, therefore one should focus on finding $F$. Recall that it is sufficient to solve the deterministic part of the problem in order to retrieve it:

\begin{align}
AX_{t-1} + BX_{t} + CX_{t+1}  = 0
\end{align}

Suppose one has a candidate solvent: $F_{n}$. Then the solution of the succeeding problem for $X_{t}$:

\begin{align}
AX_{t-1} + BX_{t} + CF_{n}X_{t}  = 0
\end{align}

is trivially given by:

\begin{align}
X_{t} = -\left( B + CF_{n} \right)^{-1} AX_{t-1}
\end{align}

Thus, the initial guess should be updated according to:

\begin{align}
F_{n+1} = -\left( B + CF_{n} \right)^{-1} A
\end{align}

Furthermore, the author proves that the above-mentioned procedure converges to the DSGE solution under the \citet{blanchard1980solution} conditions and provides an enhanced technique to deal with common situations in which solvents contain eigenvalues that are zero.

In addition, {\color{red} ADD citation} provides the simple criterion, equivalent to the  \citet{blanchard1980solution} conditions, to determine whether the obtained solution is stable and unique:

\begin{align}
\left\{
\begin{array}{cl}
\lambda(F) &< 1 \nonumber \\
\lambda \left( \left(AF + B \right)^{-1} A \right) &< 1 
\end{array}
\right.
\end{align}

where $\lambda(\cdot)$ denotes the spectral radius of a linear operator (the norm of its biggest eigenvalue).

It is worth noticing that the stability implies not only the long-term \textit{determinancy} (the model does not explode), but also the \textit{predictability} (also called \textit{non-chaoticity}), meaning that the small deviations disappear over time.

%\section{Certainty-equivalence-property}
%
%To conclude the solution section, even though agents take the effect of future shocks into account when optimizing, in a linearization to the first-order they don't matter for the decision rule.
%
%\begin{mydef}[Certainty-equivalence-property]
%In a first-order approximation the constant term needs not to be corrected for uncertainty (i.e. variance of shocks). Moreover, expectation of $x_t$ and $y_t$ is equal to its non-stochastic steady-state.
%\end{mydef}
%
%This is problematic when uncertainty does matter, not to mention the risk premium or welfare evaluation. A second-order approximation can be then implemented, thus being regarded as a potential extension.

\chapter{Estimation} \label{Estimation}

In the previous section the linear rational expectations model was solved by several different techniques, with the emphasis imposed on the Linear Time Iteration algorithm due to its differentiability, resulting in a state equation in the predetermined state variables. As the fundamental interest of the paper is to invoke the machine learning techniques to the DSGE model estimation, the formerly derived model should be written in the state space form, by adding a measurement equation linking the three observable variables (labor, consumption, investment) to the vector of state variables. Moreover, the likelihood function should be calculated using the Kalman filter, whose maximization by the Stochastic Gradient Descent leads to the parameters estimation. All three aforementioned components are described in the following sections.

\section{Kalman filter}

With unobservable state variables, one can exploit the recursive nature of the model and its fully-specified laws of motion to apply filters like the one proposed by \citet{kalman1960new}. Namely the solved model, augmented by the measurement equation, in the state space representation is as follows:

\begin{align}
X_{t} &= F_{t} X_{t-1} + G_{t} \mathbf{u}_{t}, \quad t \geqslant 0 \\
Z_{t} &= H_{t} X_{t} + \mathbf{v}_{t}
\end{align}

where $\mathbf{u}_{t}$ and $\mathbf{v}_{t}$ are independent, zero-mean Gaussian white processes with covariances:

\begin{align}
\mathbb{E} \left(\mathbf{u}_{t} \mathbf{u}_{d}^{T} \right) &= R_{t} \iota _{td} \\
\mathbb{E} \left(\mathbf{v}_{t} \mathbf{v}_{d}^{T} \right) &= Q_{t} \iota _{td}
\end{align}


Moreover, the initial state $X_{0}$ is assumed to be Gaussian random variable, independent of the noise processes, with:

\begin{align}
X_{0} \sim \mathcal{N} \left(\bar{X}_{0}, P_{0} \right)
\end{align}

In addition, let $\tilde{Z}_{t} = (Z_{0}, \ldots, Z_{t})$. The goal is to compute recursively the following optimal estimator ox $X_{t}$:

\begin{align}
\hat{X}_{t}^{+} \equiv \hat{X}_{t | t} = \mathbb{E} (X_{t} | \tilde{Z}_{t})
\end{align}

Define as well the \textit{one-step predictor} of $X_{t}$:

\begin{align}
\hat{X}_{t}^{-} \equiv \hat{X}_{t | t-1} = \mathbb{E} (X_{t} | \tilde{Z}_{t-1})
\end{align}

and the respective covariance matrices:

\begin{align}
P_{t}^{+} \equiv P_{t | t} &= \mathbb{E} \left[ \left(X_{t} -\hat{X}_{t}^{+} \right) \left(X_{t} - \hat{X}_{t}^{+} \right)^{T} |  \tilde{Z}_{t}\right] \\
P_{t}^{-} \equiv P_{t | t-1} &= \mathbb{E} \left[ \left(X_{t} -\hat{X}_{t}^{+} \right) \left(X_{t} - \hat{X}_{t}^{+} \right)^{T} |  \tilde{Z}_{t-1} \right]
\end{align}

Finally denote:

\begin{align}
P_{0}^{-} &= P_{0} \\
\hat{X}_{t}^{-} &= \bar{X}_{0}
\end{align}

From the normality, it follows that the conditional random variable $(X_{t} | \tilde{Z}_{d})$ for any $t, d$. In particular:

\begin{align}
(X_{t} | \tilde{Z}_{t}) &\sim   \mathcal{N} \left(\hat{X}_{t}^{+}, P_{t}^{+} \right) \\
(X_{t} | \tilde{Z}_{t-1}) &\sim  \mathcal{N} \left(\hat{X}_{t}^{-}, P_{t}^{-} \right)
\end{align}

In order to derive the Kalman filter, suppose, at time $t$, that $\left(\hat{X}_{t}^{-}, P_{t}^{-} \right)$ is given. Then $\left(\hat{X}_{t}^{+}, P_{t}^{+} \right)$ and $\left(\hat{X}_{t+1}^{-}, P_{t+1}^{-} \right)$ shall be computes in two steps. \\

\underline{Measurement update step:} Since $Z_{t} = H_{t} X_{t} + \mathbf{v}_{t}$, then the conditional vector $\left( \binom{X_{t}}{Z_{t}}  | \tilde{Z}_{t-1} \right)$ is Gaussian, with mean:

\begin{align}
\left[\begin{array}{c}
\hat{X}_{t}^{-} \\
H_{t} \hat{X}_{t}^{-}
\end{array}\right]
\end{align}

and covariance:

\begin{align}
\left[\begin{array}{cc}
P_{t}^{-} & P_{t}^{-}H_{t}^{T} \\
H_{t}P_{t}^{-} & M_{t}
\end{array}\right]
\end{align}

where:

\begin{align}
M_{t} = H_{t} P_{t}^{-} H_{t}^{T} + R_{t}
\end{align}

To compute $\left(X_{t} |  \tilde{Z}_{t} \right)$, it is sufficient to apply the formula for conditional expectation of Gaussian random variable, with everything pre-conditioned on $\tilde{Z}_{t-1}$. In other words, it follows that $\left(X_{t} |  \tilde{Z}_{t} \right)$ is Gaussian with mean:

\begin{align}
\hat{X}_{t}^{+} = \mathbb{E} (X_{t} | \tilde{Z}_{t} ) = \hat{X}_{t}^{-} + P_{t}^{-} H_{t}^{T} \left(M_{t} \right)^{-1} \left(Z_{t} - H_{t} \hat{X}_{t}^{-} \right)
\end{align}

and covariance:

\begin{align}
P_{t}^{+} = \mathbb{C} (X_{t} | \tilde{Z}_{t} ) = P_{t}^{-} - P_{t}^{-} H_{t}^{T}\left(M_{t} \right)^{-1} H_{t} P_{t}^{-}
\end{align}

\underline{Time update step:} Recall that $X_{t} = F_{t} X_{t-1} + G_{t} \mathbf{u}_{t}$ and that $X_{t}$ and $ \mathbf{u}_{t}$ are independent given $\tilde{Z}_{t}$. Therefore:

\begin{align}
\hat{X}_{t+1}^{-} &= \mathbb{E} (X_{t+1} | \tilde{Z}_{t} ) = F_{t} \hat{X}_{t}^{+} \\
P_{t+1}^{-} &= \mathbb{C} (X_{t+1} | \tilde{Z}_{t} ) = F_{t} P_{t}^{+} F_{t}^{T} + G_{t}Q_{t}G_{t}^{T}
\end{align} 

\section{Maximum likelihood}

Since, by construction, the forecast error from the Kalman filter is serially uncorrelated and normally distributed, it is relatively straightforward to deliver the log-likelihood (\citet{ireland2004method}). What is more, the obtained likelihood is differentiable, which enables the implementation of Automatic Differentiation in the maximization process, which in turn yields consistent and asymptotically normal estimates.

The Kalman filter allows the construction of inferences about the unobserved state vector and permits the evaluation of the joint likelihood function of observable endogenous variables. Moreover, the so-obtained likelihood becomes a differentiable functions of its parameters, allowing to implement the machine learning techniques described in Chapter \ref{MachineLearning}.

Nevertheless, as pointed out by \citet{fernandez2010econometrics}, maximizing the likelihood of the DSGE models, which in fact is a complicated and highly-dimensional function, is a tedious task. Moreover, due to the sparsity of the macroeconomics, usually quarterly data, as well as he flexibility of DSGE models in generating similar behavior with relatively different combination of parameter values, the likelihoods of DSGE models are full of local extrema and of virtually flat surfaces, which further overburden the optimization. Finally, the standard errors of the estimates are notably problematic to calculate and their asymptotic distribution inadequately approximates the small sample one. To deal some of the above-mentioned obstacles, the Bayesian interference can be introduced, as in the estimation of the \citet{smets2003estimated}.

\section{Stochastic Gradient Descent}

The computational ramification of learning algorithm becomes the decisive restricting factor when one envisions large scale machine learning problems. One of the most esteemed optimization algorithms is called Stochastic Gradient Descent (SGD, \citet{robbins1951stochastic}).

Assume that for each example $z = (x, y)$, consisting of an arbitrary input $x$ and scalar output $y$, one can construct the \textit{loss function} $l(\hat{y}, y)$ that evaluates the cost of predicting $\hat{y}$ when the actual value is $y$. The aim is to identify a function $f \in \mathcal{F}$ that minimizes the loss $L(z, w) = l(f_{w}, y)$ averaged on the examples, where $f_{w}$ is the permissible function parametrized by a weight vector $w$. In fact, one desires to average over the unknown distribution $dP(z)$:

\begin{align}
\mathbf{E} (f) = \int l(f(x), y) dP(z)
\end{align}

when only the average on the sample $z_{1}, \ldots, z_{n}$ is available:

\begin{align}
\mathbf{E}_{n} = \frac{1}{n} \sum\limits_{i=1}^{n} l(f(x_{i}), y_{i})
\end{align}

The \textit{empirical risk} $\mathbf{E}_{n}$ measures the training set performance. The \textit{expected risk} $\mathbf{E} (f)$ regulates the generalization performance, defined as the expected performance on future examples. The statistical learning theory (\citet{vapnik2015uniform}) countenances minimizing the empirical risk instead of the expected one when the selected family F is sufficiently restrictive.

It has repeatedly been suggested (\citet{rumelhart1985learning}) to minimize the empirical risk $\mathbf{E} (f_{w})$ using Gradient Descent (GD). Each iteration updates the weights $w$ on the basis of the gradient of $\mathbf{E}_{n} (f_{w})$:

\begin{align}
w_{t+1} = w_{t} - \gamma \frac{1}{n} \sum\limits_{i=1}^{n} \nabla_{w} L(z_{i}, w_{t})
\end{align}

where $\gamma$ is a competently chosen learning rate. Under sufficient regularity assumptions, when the initial estimate $w_{0}$ is close enough to the optimum and when the learning rate $\gamma$ is amply modest, this algorithm accomplishes linear convergence (\citet{dennis1996numerical}).

The Stochastic Gradient Descent (SGD) algorithm is a drastic simplification. Instead of computing the gradient of $\mathbf{E}_{n} (f_{w})$ exactly, each iteration estimates it on the basis of a single randomly picked example $z_{i}$:

\begin{align}
w_{t+1} = w_{t} - \gamma \nabla_{w} L(z_{i}, w_{t})
\end{align}

As a result, SGD is far less computationally demanding than GD. On the other hand, although SGD minimizes loss faster, it is noisier and it oscillates around the the minimum giving some variation in accuracy and loss run to run (\citet{bottou2012stochastic}).

\chapter{Results}

The small-sample properties of Maximum Likelihood estimator augmented with machine learning are studied here using Monte Carlo analysis .All experiments are based on 100 replications using a sample size of 200 observations, corresponding to the set of quarterly observations of the series for a period of 50 years. In order to restrict the impact of the initial values used to generate the series, 100 extra observations were produced in every single replication. Then, for the estimation part, the starting 100 observations were repealed.

In order to limit the computational burden in the Monte Carlo experiments, the author concentrates on up to five (of the thirty two) model parameters, namely discount rate, Calvo prices stickiness, standard deviation of government spending shock, capital income share and depreciation rate, keeping the rest fixed. The parameters values used for the simulation are presented in Table \ref{True_par_values}.

\newpage

\begin{table}[H]
\centering
\caption[True parameters values]{True parameters values}
    \label{True_par_values}
\small{
\begin{tabular}{@{}| c | l | c |@{}}
\multicolumn{3}{l}{Behavioral parameters} \\
\midrule
$\psi$ & Investment adjustment cost & 1/0.169 \\
$\sigma_L$ & Labour utility & 2.4 \\
$\sigma_c$ & Consumption utility & 1.353 \\
$h$ & Consumption habit & 0.573 \\
$\phi$ & Fixed cost & 1.408 \\
$adj$ & Capital utilization adjustment cost & 1/6.771 \\
$\beta$ & Discount factor & 0.99 \\
\midrule
\multicolumn{3}{l}{}  \\
\multicolumn{3}{l}{Wage and prices parameters}\\
\midrule
$\gamma_p$ & Indexation prices & 0.469 \\
$\gamma_w$ & Indexation wages & 0.763 \\
$\varsigma_{p}$ & Calvo prices & 0.908 \\
$\varsigma_{w}$ & Calvo wages & 0.737 \\
\midrule
\multicolumn{3}{l}{}  \\
\multicolumn{3}{l}{Monetary policy function parameters} \\
\midrule
$r_\pi$ & Inflation & 1.684 \\
$r_{d\pi}$ & Difference of inflation & 0.14 \\
$r_Y$ & Output & 0.099 \\
$r_{dY} $ & Difference of output & 0.159 \\
$\lambda_w$ & Calvo employment & 0.5 \\
$\rho$ & Lagged interest rate & 0.961 \\
\midrule
\multicolumn{3}{l}{}  \\
\multicolumn{3}{l}{Persistence of shocks} \\
\midrule
$\rho_L$ & Labour supply shock & 0.889 \\
$\rho_a$ & Productivity shock & 0.823 \\
$\rho_b$ & Consumption preference shock & 0.855 \\
$\rho_G$ & Government spending shock & 0.949 \\
$\rho_\pi$ & Inflation objective shock & 0.924 \\
$\rho_I$ & Investment shock & 0.927 \\
\midrule
\multicolumn{3}{l}{}  \\
\multicolumn{3}{l}{Innovations of the shock} \\
\midrule
$sd_R$ & Interest rate shock & 0.081 \\
$sd_p$ & Price mark-up shock & 0.16 \\
$sd_w$ & Wage mark-up shock & 0.289 \\
$sd_Q$ & Equity premium shock & 0.604 \\
\midrule
\multicolumn{3}{l}{}  \\
\multicolumn{3}{l}{Other parameters} \\
\midrule
$\tau$ & Depreciation rate & 0.025 \\
$\alpha$ & Capital income share & 0.3 \\
$r_k$ & Rental rate of capital & 0.078 \\
$k$ & Steady state capital-output ratio & 8.8 \\
$g$ & Steady state government spending-output ratio & 0.18 \\
\bottomrule
\end{tabular}}
\end{table}

Moreover, as the maximum likelihood algorithm requires the initial guess for the estimated parameters, the draws from following intervals, widely accepted in the DSGE literature, are considered:

\begin{table}[H]
\centering
\caption[Intervals for initial guess of parameters values]{Initial guesses of parameters values}
    \label{Initial_par_values}
\begin{tabular}{@{}| c | l | c | c |@{}}
\toprule
Parameter & Description & Lower bound & Upper bound \\
\midrule
$\beta$ & Discount factor & 0.90 & 0.99 \\
$\varsigma_{p}$ & Calvo prices & 0.90 & 0.99 \\
$\rho_G$ & Government spending shock & 0.90 & 0.99 \\
$\alpha$ & Capital income share & 0.2 & 0.4 \\
$\tau$ & Depreciation rate & 0.025 & 0.05 \\
\bottomrule
\end{tabular}
\end{table}

Moreover, due to the suspicion of the existence of local maxima, the multiple starting points are selected during each iteration and the final value is chosen as one maximizing the obtained likelihoods.

\section{Preeliminary results}

Before conducting the main Monte Carlo analysis, the author decided to test the algorithm on the estimation of one specific parameter: the discount factor. Being more specific, it is useful to test the dependance on the initial guess matrix in the Linear Time Iteration algorithm, with particular emphasis imposed on the duration of the execution and its closeness to the real parameter value. Moreover, it is beneficial to verify the reliance on the performed number of optimization steps, as well as the amount of starting values used in the calculations. 

\begin{table}[!htbp] \label{Beta_stress}
\centering
\caption{Beta estimation stress testing}
\begin{tabularx}{\textwidth}{@{} M *{3}{Y} M M L  @{}}
\toprule
F matrix & $n_{start}$ & $n_{opt}$ & $\beta$ & Std. Error & Time
\\ \midrule
Zeros              & 100                          & 100                       &     0.9893     &           3.9111e-09                         &                                          11 100s  ($\sim$ 3h)       \\
Zeros              & 1000                         & 100       &    0.9893            &                           3.8888e-09                   &                                    113 100s  ($\sim$ 31h)     \\
Zeros              & 100                          & 1000                      &   0.9894       &               2.9755e-09                     &                     75 600s    ($\sim$ 24h)                          \\
Correct F          & 100                          & 100                       &     0.9894     &         6.2042e-09                           &                              7 800s         ($\sim$ 2h)               \\
Correct F          & 1000                         & 100                       &     0.9894     &                 4.6190e-09                   &                                  77 100s          ($\sim$ 22h)           \\
Correct F          & 100                          & 1000                      &        0.9896  &           3.8645e-09                            &                                                79 400s ($\sim$ 22h)                   \\ \bottomrule
\end{tabularx}
\caption*{\small{Where the columns denote respectively guessed matrix $F$, number of starting points, number of optimization steps, $\beta$ estimate, standard error and execution time.}}
\end{table}

The maximization of likelihood function during the scenarios listed in Table \ref{Beta_stress} points out that even the elementary scheme with 100 optimization steps and exactly the same number of starting points can lead to the acceptable estimate. Moreover, the results clearly promotes the implementation of F matrix with true parameters as the initial guess in the Linear Time Iteration algorithm, thus the same technique is adopted in the Monte Carlo analysis.

On the other hand, the standard errors, also reported in Table \ref{Beta_stress}, correspond to the square roots of the diagonal elements of negative of the inverted matrix of second derivatives of the maximized log-likelihood function. Generally, calculating these standard errors requires two steps, numerically evaluating the matrix of second derivatives of the log-likelihood function and then inverting that very large matrix having elements of varying magnitudes, both of which may introduce approximation error into the statistics (\citet{iskrev2008evaluating}). In spite of the fact that the automatic differentiation procedure provides the Hessian matrix relatively costlessly, significantly reducing the computational tine, its inversion still may be problematic. Hence, these standard errors, though useful, do need to be interpreted with a bit of caution.

Finally, increasing both the amount of optimization steps and the number of starting points lead directly to the better estimate, simultaneously dramatically increasing the computational time. Therefore, the elementary scheme (100, 100) should be firstly considered to be adopted in the Monte Carlo analysis. Nonetheless, in furtherance of performing at least 100 Monte Carlo trials, the amount of starting points is diminished to 10.

\section{Monte Carlo results}

In the following section the results of 100 Monte Carlo trials of estimation of the DSGE model in different scenarios are illustrated. Similar scheme to the previous chapter is the first to consider, namely 100 Monte Carlo trials of discount factor estimation performed with 10 starting points and 100 optimization steps.

\begin{figure}[H] \centering
\includegraphics[width=\textwidth]{Hist_1}
\caption[Histogram of one parameter estimation]{Histogram of one parameter estimation}
\label{Hist_1}
\end{figure} 

The histogram \ref{Hist_1} confirms that the implemented estimation method lead to the relevant results, nevertheless as can be observed in the upper part of Table \ref{True_par_values} it demands a significant portion of time and computational power.

\begin{table}[!htbp]
\centering
\caption[Results of estimation]{Results of estimation}
    \label{Results}
\begin{tabularx}{\textwidth}{@{} | Y Y Y Y  M  | @{}}
\multicolumn{5}{l}{Single parameter estimation} \\
\midrule
Parameter & True value & Estimation & Std. Error  & Time \\
\midrule
$\beta$ & 0.99 &  0.9878 & 1.5356e-09 &  750s ($\sim$ 0.20h) \\
\midrule
\multicolumn{5}{l}{}  \\
\multicolumn{5}{l}{Three parameters estimation}\\
\midrule
Parameter & True value & Estimation & Std. Error & Time \\
\midrule
$\beta$ & 0.99&  0.9872 & 2.4556e-09 & 900s ($\sim$ 0.25h) \\
$\varsigma_p$ & 0.908 &0.9650  &7.3073e-09 &\\
$\rho_G$  & 0.949 & 0.9759 &4.6946e-09 &\\
\midrule
\multicolumn{5}{l}{}  \\
\multicolumn{5}{l}{Five parameters estimation} \\
\midrule
Parameter & True value & Estimation & Std. Error  & Time \\
\midrule
$\beta$ & 0.99& 0.9833 & 2.6776e-09 & 1500s ($\sim$ 0.30 h) \\
$\varsigma_{p}$& 0.908  & 0.9674 & 8.0667e-09 & \\
$\rho_G$ &  0.949 & 0.9678 & 5.8213e-09 &\\
$\alpha$ &  0.3 & 0.3983 &1.8797e-09 &\\
$\tau$ & 0.025 & 0.0309 & 1.3171e-09  &\\
\bottomrule
\end{tabularx}
\caption*{\small{Where the columns denote respectively the parameter name, mean estimate, average standard error and average execution time (per one Monte Carlo iteration).}}
\end{table}

As the next step, the Monte Carlo trials with identical framework are performed, trying to estimate three and five parameters at once respectively. It can be observed that the $\beta$ estimate is further from the true value, pointing out that the convergence in higher dimensional space is much more difficult to obtain in the same amount of steps. Moreover, the obtained values are wider spread, as can be deducted from the inspection of Figure \ref{Hist_3}. Similarly, the average estimates of two other parameters, namely $\varsigma_{p}$ and $\rho_G$ are far from optimal, while the computational time increases significantly.

\begin{figure}[!htbp] \centering
\includegraphics[width=\textwidth]{Hist_3}
\caption[Histograms of three parameters estimation]{Histograms of three parameters estimation}
\label{Hist_3}
\end{figure} 

\newpage

Finally, the simultaneous estimation of five parameters confirms all of the above-mentioned findings. Particularly important is the time dimension, which significant in absolute value, does not increases linearly. In other words, one may assume that the estimation procedure outlined in the paper should be especially considered in the high-dimensional estimation problems.

\begin{figure}[H] \centering
\includegraphics[width=\textwidth]{Hist_all}
\caption[Histograms of five parameters estimation]{Histograms of five parameters estimation}
\label{Hist_all}
\end{figure} 

It has to be accentuated that the choice of observable variables employed in Maximum Likelihood estimation matters (\citet{ruge2007methods}). Therefore replication of the performed exercise with different set of observables may be seen as a potential extension. Nevertheless, the obtained results clearly indicates that the Machine-Learning-enhanced Maximum Likelihood estimation of the DSGE model should be considered as a helpful alternative to the Bayesian frameworks only in the case of highly-dimensional problems, with a plethora of parameters to estimate.

\chapter{Concluding remarks} \label{Concluding remarks}

With the quick advance on computers verified on the past few years, Dynamic Stochastic General Equilibrium framework became an important tool in macro-analysis. Moreover, recently one can observe an expeditious developments in the Machine Learning domain. Therefore the paper aimed to implement the progressive achievements of ML to the medium-size DSGE model estimation. 

Performed Monte Carlo analysis evidently demonstrated that the proposed framework can be cogitated as an effective substitute to the Bayesian interference in case of highly- dimensional space parameter estimation. Undoubtedly, the combination of Machine Learning and DSGE models has a great perspectives of growth in the near future.However, there are important gaps and uncertainties that remain to be solved.

Thus, the code implementation on the Graphics Processing Unit can be regarded as a valuable extension, potentially disentangling the speed and memory allocation issues. In other words, rewriting code to be run on the GPU may significantly boost the algorithms performance even 10 times (\citet{michalakes2008gpu}). Moreover, the Maximum Likelihood estimation may surpass the classical Bayesian framework in the case of higher approximations of the solution function, incorporating non-linearities. 

\newpage

\nocite{*}
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{Dissertation}

\newpage

\chapter*{Appendix A} \label{Appendix A}
\addcontentsline{toc}{chapter}{Appendix A}

\section*{Consumption and savings behavior}

The maximization of the objective function of the household subject to the budget constraint with respect to consumption and holdings of bonds, may be performed through the Lagrangian-based technique:

\begin{equation}
\begin{split}
L = \mathbb{E}_{0} \sum\limits_{t=0}^{\infty} \beta^{t} \varepsilon_{t}^{B} \left[ \frac{1}{1-\sigma_{c}} \left( C_{t}^{\tau} - H_{t}^{\tau} \right)^{1-\sigma_{c}} - \frac{\varepsilon_{t}^{L}}{1+\sigma_{l}} \left( l_{t}^{\tau} \right)^{1+\sigma_{l}} \right] + \\
- \beta^{t} \lambda_{t} \left[ C_{t}^{\tau} + I_{t}^{\tau} + b_{t} \frac{B_{t}^{\tau}}{P_{t}} - Y_{t}^{\tau} - \frac{B_{t-1}^{\tau}}{P_{t}} \right]
\end{split}
\end{equation}

The derivatives with respect to $C_{t}^{\tau}$ and $B_{t}^{\tau}$ are respectively:

\begin{align}
\frac{\partial L}{\partial C_{t}^{\tau}} &= \mathbb{E}_{t} \left[ \beta^{t} \left( \varepsilon_{t}^{B} (
C_{t}^{\tau} - H_{t} )^{-\sigma_{c}} - \lambda_{t} \right) \right]  = 0  \nonumber \\ 
& \iff \lambda_{t} =  \varepsilon_{t}^{B} (C_{t}^{\tau} - H_{t} )^{-\sigma_{c}} = \frac{\partial U_{t}^{\tau}}{\partial C_{t}^{\tau}} := U_{t}^{c} \label{FOC_C} \\
\frac{\partial L}{\partial B_{t}^{\tau}} &= -\mathbb{E}_{t} \left[ \beta^{t} \lambda_{t} \frac{b_{t}}{P_{t}} \right] - \mathbb{E}_{t} \left[ \beta^{t+1} \lambda_{t+1} \frac{-1}{P_{t+1}} \right] = 0 \nonumber \\
& \iff \mathbb{E}_{t} \left[ \beta \frac{\lambda_{t+1}}{\lambda_{t}} \frac{1}{b_{t}} \frac{P_{t}}{P_{t+1}} \right] = 1 \label{FOC_B} 
\end{align}

Exploiting the fact that households are homogeneous in their consumption-saving decisions,  meaning that the marginal utility of consumption is identical for all $\tau$, yields the following Euler equation:

\begin{align}
\mathbb{E}_{t} \left[ \beta \frac{\lambda_{t+1}}{\lambda_{t}} \frac{1}{b_{t}}\frac{P_{t}}{P_{t+1}} \right] &= \mathbb{E}_{t} \left[ \beta \frac{U^{c}_{t+1}}{U^{c}_{t}} (1 + i_{t}) \frac{P_{t}}{P_{t+1}} \right] = \nonumber \\
&= \mathbb{E}_{t} \left[ \beta \frac{\varepsilon_{t+1}^{B} (C_{t+1}^{\tau} - H_{t+1} )^{-\sigma_{c}}}{\varepsilon_{t}^{B} (C_{t}^{\tau} - H_{t} )^{-\sigma_{c}}} R_{t} \frac{P_{t}}{P_{t+1}} \right] = \nonumber \\
&= \mathbb{E}_{t} \left[ \beta \frac{\varepsilon_{t+1}^{B} (C_{t+1}^{\tau} - h \cdot C_{t} )^{-\sigma_{c}}}{\varepsilon_{t}^{B} (C_{t}^{\tau} - h \cdot C_{t-1} )^{-\sigma_{c}}} R_{t} \frac{P_{t}}{P_{t+1}} \right] = 1
\end{align}

Alternatively one can rearrange the aforementioned equation as:
\begin{align}
\mathbb{E}_{t} \left[ \beta \frac{\lambda_{t+1}}{\lambda_{t}} \frac{1}{b_{t}}\frac{P_{t}}{P_{t+1}} \right] &= \mathbb{E}_{t} \left[ \beta \frac{U^{c}_{t+1}}{U^{c}_{t}} (1 + i_{t}) \frac{P_{t}}{P_{t+1}} \right] = 1 \nonumber \\
& \iff \beta (1 + i_{t}) \mathbb{E}_{t} \left[ \frac{U_{t+1}^{c}}{P_{t+1}} \right] = \frac{U_{t}^{c}}{P_{t}}
\end{align}

\section*{Labor supply decisions}

The problem of the firm is as following:

\begin{align}
\min_{l_{t}^{\tau}} \int\limits_{0}^{1} W_{t}^{\tau} l_{t}^{\tau} d\tau \quad \text{s.t.} \quad L_{t} = \left[ \int\limits_{0}^{1} \left( l_{t}^{\tau} \right)^{\frac{1}{1+\lambda_{w,t}}} d\tau \right]^{1+\lambda_{w,t}}
\end{align}

Define $W_{t}$ as the Lagrange multiplier, then the cost minimization becomes:

\begin{align}
L = \int\limits_{0}^{1} W_{t}^{\tau} l_{t}^{\tau} d\tau + W_{t} \left( L_{t} - \left[ \int\limits_{0}^{1} \left( l_{t}^{\tau} \right)^{\frac{1}{1+\lambda_{w,t}}} d\tau \right]^{1+\lambda_{w,t}} \right)
\end{align}

Taking the first order conditions:

\begin{align} \label{Optimal demand function for labor}
\frac{\partial L}{\partial l_{t}^{\tau}} &= W_{t}^{\tau} - W_{t} \left( \left( 1+\lambda_{w ,t} \right) \left[  \int\limits_{0}^{1} \left( l_{t}^{\tau} \right)^{\frac{1}{1+\lambda_{w ,t}}} d\tau \right]^{\lambda_{w ,t}} \frac{1}{1+\lambda_{w ,t}}  \left( l_{t}^{\tau} \right)^{\frac{-\lambda_{w ,t}}{1+\lambda_{w ,t}}} \right) = 0 \nonumber \\
& \iff W_{t}^{\tau} = W_{t} L_{t}^{\frac{\lambda_{w ,t}}{1+\lambda_{w,t}}} \left( l_{t}^{\tau} \right)^{\frac{-\lambda_{w,t}}{1+\lambda_{w ,t}}} \nonumber \\
& \iff l_{t}^{\tau}= L_{t} \left( \frac{W_{t}^{\tau}}{W_{t}}\right)^{\frac{-(1+\lambda_{w,t})}{\lambda_{w ,t}}}
\end{align} 

The derived optimal demand function of labor can be then substituted in the aggregate labor demand equation yielding:

\begin{align}
L_{t} &= \left[ \int\limits_{0}^{1} \left( \frac{W_{t}^{\tau}}{W_{t}}\right)^{\frac{-1}{\lambda_{w ,t}}} L_{t}^{\frac{1}{1+\lambda_{w ,t}}} d\tau \right]^{1+\lambda_{w,t}} \nonumber \\
& \iff W_{t} = \left[ \int\limits_{0}^{1} \left( W_{t}^{\tau} \right)^{\frac{-1}{\lambda_{w ,t}}}  d\tau \right]^{-\lambda_{w,t}}
\end{align}

The Lagrange multiplier $W_{t}$, generally considered as the representation of the marginal value of relaxing the constrain, can be interpreted as well as the price of a working hour and thus a wage index.

\section*{Wage setting equation}

Households that can re-set their wages maximize their objective function subject to their budget constraint and the demand of labor, taking into consideration the fact that wages remain fixed until period $i$ with a probability $\left( \varsigma_{w} \right)^{i}$. The Lagrangian function in t becomes:

\begin{align}
L = \mathbb{E}_{t} \sum\limits_{i=0}^{\infty} &\beta^{i} \varsigma_{w}^{i} \left\{\underbrace{U \left(C_{t+i}^{\tau}, l_{t+i}^{\tau} \right)}_\text{Utility function} - \lambda_{t+i} \left( \underbrace{\ldots + \frac{W_{t+i}^{\tau}}{P_{t+i}} l_{t+i}^{\tau} + \ldots }_\text{Budget constraint} \right) + \nonumber \right. \\ 
& \left. - \mu_{t+i} \left( \underbrace{l_{t+i}^{\tau} - L_{t+i}^{\tau} \left(\frac{W_{t+i}^{\tau}}{W_{t+i}} \right)^{\frac{-\left(1-\lambda_{w, t+i}\right)}{\lambda_{w, t+i}}}}_\text{Labor demand} \right) \right\}
\end{align}

Substituting the demand of labour (\ref{Optimal demand function for labor}) in the objective function and in the budget constraint and considering the formulas for not reoptimized wages in period $t+i$, one obtains:

\begin{align}
L = \mathbb{E}_{t} \sum\limits_{i=0}^{\infty} &\beta^{i} \varsigma_{w}^{i}  \left\{ \underbrace{U \left(C_{t+i}^{\tau}, L_{t+i} \left( \frac{\left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \widetilde{W}_{t}}{W_{t+i}}\right)^{\frac{-(1+\lambda_{w,t+i})}{\lambda_{w,t+i}}} \right)}_\text{Utility function} + \right. \nonumber \\
& \left. - \lambda_{t+i} \left( \underbrace{\ldots + \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \frac{\widetilde{W}_{t}}{P_{t+i}} L_{t+i} \left( \frac{\left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \widetilde{W}_{t}}{W_{t+i}}\right)^{\frac{-(1+\lambda_{w,t+i})}{\lambda_{w,t+i}}}
 + \ldots }_\text{Budget constraint} \right) \right\}
\end{align}

Taking the first order conditions:

\begin{align}
\frac{\partial L}{\partial \widetilde{W}_{t}}  =  \mathbb{E}_{t} \sum\limits_{i=0}^{\infty} &\beta^{i} \varsigma_{w}^{i}  \left\{ U_{t+i}^{L} {\frac{-(1+\lambda_{w,t+i})}{\lambda_{w,t+i}}} L_{t+i} \left( \frac{\left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \widetilde{W}_{t}}{W_{t+i}}\right)^{\frac{-(1+\lambda_{w,t+i})}{\lambda_{w,t+i}}-1} \left( \frac{\left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}}}{W_{t+i}}\right)  \right. + \nonumber \\ 
-  &\lambda_{t+i} \left[ \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \frac{1}{P_{t+i}} L_{t+i} \left( \frac{\left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \widetilde{W}_{t}}{W_{t+i}}\right)^{\frac{-(1+\lambda_{w,t+i})}{\lambda_{w,t+i}}} \right. + \nonumber \\ 
-  &\left. \left. \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \frac{\widetilde{W_{t}}}{P_{t+i}} L_{t+i} {\frac{1+\lambda_{w,t+i}}{\lambda_{w,t+i}}} \left( \frac{\left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \widetilde{W}_{t}}{W_{t+i}}\right)^{\frac{-(1+\lambda_{w,t+i})}{\lambda_{w,t+i}}-1} \left( \frac{\left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}}}{W_{t+i}}\right) \right] \right\} = 0
\end{align}

Therefore:

\begin{align}
\frac{\partial L}{\partial \widetilde{W}_{t}} = &\mathbb{E}_{t} \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{w}^{i}  \left\{ U_{t+i}^{L} {\frac{-(1+\lambda_{w,t+i})}{\lambda_{w,t+i}}} \frac{l_{t+1}^{\tau}}{\widetilde{W}_{t}}  \right. + \nonumber \\ 
- &\lambda_{t+i} \left. \left[ \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \frac{1}{P_{t+i}} l_{t+i}^{\tau} - {\frac{1+\lambda_{w,t+i}}{\lambda_{w,t+i}}} \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \frac{1}{P_{t+i}} l_{t+i}^{\tau} \right] \right\} = \nonumber \\ 
= &\mathbb{E}_{t} \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{w}^{i}  \left\{ U_{t+i}^{L} {\frac{-(1+\lambda_{w,t+i})}{\lambda_{w,t+i}}} \frac{l_{t+1}^{\tau}}{\widetilde{W}_{t}}  \right. + \nonumber \\ 
- &\lambda_{t+i} \left. \left[ {\frac{-1}{\lambda_{w,t+i}}} \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}} \frac{1}{P_{t+i}} l_{t+i}^{\tau} \right] \right\} = 0
\end{align}

Multiplying by the factor $\frac{-\lambda_{w,t+i}}{1+\lambda_{w,t+i}}$ and considering that $\lambda_{t} = U_{t}^{c}$ one obtains:

\begin{align}
\frac{\widetilde{W}_{t}}{P_{t}} &\mathbb{E}_{t} \left\{ \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{w}^{i} \left( \frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{w}}  \frac{P_{t}}{P_{t+i}} \left( \frac{l_{t+1}^{\tau} U_{t+i}^{C}}{1 + \lambda_{w, t+i}} \right) \right\} = \nonumber \\
&= \mathbb{E}_{t} 
\left\{ \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{w}^{i} l_{t+1}^{\tau} U_{t+i}^{l} \right\}
\end{align}

Assuming perfect flexibility of wages $(\varsigma_{w} = 0)$, the above-mentioned equation becomes:

\begin{align}
\frac{\widetilde{W}_{t}}{P_{t}} = \left(1 + \lambda_{w, t} \right)\frac{U_{t}^{L}}{U_{t}^{C}}
\end{align}

\section*{Capital accumulation and investment decision}

Define the Lagrange multiplier $\lambda_{t} Q_{t}$ for the maximization problem with respect to $K_{t}, I_{t}, z_{t}$:

\begin{align}
L = \mathbb{E}_{0} \sum\limits_{t=0}^{\infty} &\beta^{t} \left[\underbrace{U \left(C_{t}^{\tau}, l_{t}^{\tau} \right)}_\text{Objective function} - \lambda_{t} \left( \underbrace{I_{t} + \left(\Psi(z_{t}) - r_{t}^{k}z_{t}\right)K_{t-1} + \ldots}_\text{Budget constraint} \right) + \nonumber \right.  \\ 
& \left. - \lambda_{t} Q_{t} \left( \underbrace{K_{t} - K_{t-1} (1 - \tau) - I_{t}  + I_{t} \cdot S \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right)}_\text{Capital accumulation} \right) \right]
\end{align}

The first-order conditions:

\begin{align}
\frac{\partial L}{\partial z_{t}} &= \mathbb{E}_{t} \left[ -\beta^{t} \lambda_{t} \left( \Psi'(z_{t}) - r_{t}^{k} \right) K_{t-1} \right] = 0 \nonumber \\
& \iff r_{t}^{k} = \Psi'(z_{t})
\end{align}

The capital utilization rate is set so that the revenue $r_{t}^{k}$ of the marginal utilization equals the marginal costs $\Psi'(z_{t})$.

\begin{align}
\frac{\partial L}{\partial K_{t}} &= \mathbb{E}_{t} \left[ \beta^{t+1} \lambda_{t+1} \left( z_{t+1}r_{t+1}^{k} - \Psi(z_{t+1}) \right) - \beta^{t} \lambda_{t}Q_{t} + \beta^{t+1} \lambda_{t+1}Q_{t+1}(1-\tau) \right] = 0 \nonumber \\
& \iff Q_{t} = \mathbb{E}_{t} \left[ \beta \frac{\lambda_{t+1}}{\lambda_{t}} \left( Q_{t+1}(1-\tau) + z_{t+1}r_{t+1}^{k} - \Psi(z_{t+1}) \right) \right]
\end{align}

The real value of the stock of capital today $\lambda_{t} Q_{t}$ is equal to the expected value of sum of not depreciated stock of capital of the next period $\lambda_{t+1}Q_{t+1}(1-\tau)$ and the expected revenue of the future utilization $ z_{t+1}r_{t+1}^{k}$ minus the related costs $\Psi(z_{t+1})$.

\begin{align}
\frac{\partial L}{\partial I_{t}} &= \mathbb{E}_{t} \left[ -\beta^{t} \lambda_{t} - \beta^{t} \lambda_{t} Q_{t} \left(-1 + S\left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) + I_{t} \cdot S' \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \cdot \frac{\varepsilon_{t}^{I}}{I_{t-1}} \right) \right] + \nonumber \\
\qquad &- \mathbb{E}_{t} \left[ \beta^{t+1} \lambda_{t+1} Q_{t+1} \left(I_{t+1} \cdot S' \left(\frac{\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}} \right) \cdot \frac{-\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}^{2}} \right) \right] = 0
\end{align}

The costs of the marginal investment (included the adjustment costs) must be equal to the expected marginal revenue of investment.

\section*{Firms - Final product sector}

The Lagrangian for the minimization problem:

\begin{align}
\int\limits_{0}^{1} p_{t}^{j} y_{t}^{j}dj + P_{t} \left( Y_{t} - \left[ \int\limits_{0}^{1} \left(y_{t}^{j} \right)^\frac{1}{1+\lambda_{p,t}} dj \right]^{1+\lambda_{p,t}} \right)
\end{align}

First order conditions:

\begin{align}
\frac{\partial L}{\partial y_{t}^{j}} &= p_{t}^{j} - P_{t} \left( \left( 1+\lambda_{p,t} \right) \left[ \int\limits_{0}^{1} \left(y_{t}^{j} \right)^\frac{1}{1+\lambda_{p,t}} dj \right]^{\lambda_{p,t}} \frac{1}{1+\lambda_{p,t}} \left( y_{t}^{j} \right)^{ \frac{-\lambda_{p,t}}{1+\lambda_{p,t}}} \right) = 0 \nonumber \\
&  \iff p_{t}^{j} = P_{t}  Y_{t}^{ \frac{\lambda_{p,t}}{1+\lambda_{p,t}}} \left( y_{t}^{j} \right)^{ \frac{-\lambda_{p,t}}{1+\lambda_{p,t}}} \nonumber \\
& \iff y_{t}^{j} = Y_{t} \left( \frac{p_{t}^{j}}{P_{t}} \right)^{ \frac{-(1+\lambda_{p,t})}{\lambda_{p,t}}} 
\end{align}

Substituting into the technology constraint:

\begin{align}
Y_{t} &=  \left[ \int\limits_{0}^{1} \left( \frac{ p_{t}^{j}}{P_{t}} \right)^{\frac{-1}{\lambda_{p,t}}} Y_{t}^\frac{1}{1+\lambda_{p,t}} dj \right]^{1+\lambda_{p,t}} \nonumber \\
& \iff P_{t} = \left[ \int\limits_{0}^{1} \left( p_{t}^{j} \right)^{\frac{-1}{\lambda_{p,t}}} dj \right]^{-\lambda_{p,t}}
\end{align}


\section*{Firms - Intermediate product sector}

In order to find the optimal production quantity, it is convenient to show that marginal costs are constant. One proceeds first by finding a functional relationship between $\widetilde{K}_{j,t}$ and $L_{j,t}$ derived by the minimization problem of $W_{t}L_{j,t} + r_{t}^{k} \widetilde{K}_{j,t}$ with respect to $L_{j,t}$ and $\widetilde{K}_{j,t}$ subject to $y_{t}^{j} = \varepsilon_{t}^{a} \widetilde{K}_{j,t}^{\alpha} L_{j,t}^{1-\alpha} - \Phi$.

\begin{align}
\frac{W_{t}}{r_{t}^{k}} &= \frac{(1-\alpha)\varepsilon_{t}^{a} \widetilde{K}_{j,t}^{\alpha} L_{j,t}^{-\alpha} }{\alpha \varepsilon_{t}^{a} \widetilde{K}_{j,t}^{\alpha-1} L_{j,t}^{1-\alpha} } \nonumber \\
& \iff \frac{W_{t}L_{t,j}}{r_{t}^{k}  \widetilde{K}_{j,t}} = \frac{1-\alpha}{\alpha}
\end{align}

The utilization rate between capital and labour is identical for all firms and constant. This is the same of the economy as a whole. The marginal costs of product $j$ are derived by substituting the above-mentioned relation in the production function:

\begin{align}
\widetilde{K}_{j,t} &= \left(y_{t}^{j} + \Phi \right) \frac{1}{\varepsilon_{t}^{a}} \left(\frac{\alpha}{1-\alpha} \frac{W_{t}}{r_{t}^{k}} \right)^{1-\alpha} \\
L_{j,t} &= \left(y_{t}^{j} + \Phi \right) \frac{1}{\varepsilon_{t}^{a}} \left(\frac{\alpha}{1-\alpha} \frac{W_{t}}{r_{t}^{k}} \right)^{-\alpha}
\end{align}

Therefore the total cost equals:

\begin{align}
TC_{t} = W_{t}L_{j,t}  + r_{t}^{k} \widetilde{K}_{j,t} =  \left(y_{t}^{j} + \Phi \right) \frac{1}{\varepsilon_{t}^{a}} W_{t}^{1-\alpha} \left(r_{t}^{k} \right)^{\alpha} \left( \alpha^{-\alpha} (1-\alpha)^{-(1-\alpha)}\right)
\end{align}

Hence the marginal cost takes the form:

\begin{align}
MC_{t} = \frac{\partial TC_{t}}{\partial y_{t}^{j}} = \frac{1}{\varepsilon_{t}^{a}} W_{t}^{1-\alpha} \left(r_{t}^{k} \right)^{\alpha} \left( \alpha^{-\alpha} (1-\alpha)^{-(1-\alpha)}\right)
\end{align}

In order to derive the optimal mark-up formula for the reoptimize price, the Lagrangian can be formed:

\begin{align}
L = \mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \rho_{t+i} \left\{ \left[
\left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}\widetilde{p}_{t} - MC_{t+i}
\right] Y_{t+i} \left[ \frac{\left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}\widetilde{p}_{t}}{P_{t+i}} \right]^{\frac{-(1+\lambda_{p,t+i})}{\lambda_{p,t+i}}} - \Phi MC_{t+i} \right\}
\end{align}

First order condition:

\begin{align}
\frac{\partial L}{\partial \widetilde{p}_{t}} &= \mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \rho_{t+i} \left\{ \left[
\left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}} \right] Y_{t+i} \left[ \frac{\left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}\widetilde{p}_{t}}{P_{t+i}} \right]^{\frac{-(1+\lambda_{p,t+i})}{\lambda_{p,t+i}}} \right. + \nonumber \\
& \left. + \left[\left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}\widetilde{p}_{t} -MC_{t+i} \right] Y_{t+i} \frac{-(1+\lambda_{p,t+i})}{\lambda_{p,t+i}} \left[ \frac{\left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}\widetilde{p}_{t}}{P_{t+i}} \right]^{\frac{-(1+\lambda_{p,t+i})}{\lambda_{p,t+i}}-1} \frac{\left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}}{P_{t+i}} \right\} = \nonumber \\
& = \mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \rho_{t+i} \left\{ \left[ \left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}} \right] y^{j}_{t+i} + \left[\left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}\widetilde{p}_{t} -MC_{t+i} \right] \frac{-(1+\lambda_{p,t+i})}{\lambda_{p,t+i}} \frac{y_{t+i}^{j}}{\widetilde{p}_{t}} \right\} = 0
\end{align}

Multiplying through the factor $\widetilde{p}_{t} \lambda_{p,t+i}$ and considering equation \ref{Euler for firm}, the following mark-up formula for the reoptimized price obtains:

\begin{align}
&\mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \frac{\lambda_{t+i}}{\lambda_{t}} y_{t+i}^{j} \left\{ \frac{\widetilde{p}_{t}}{P_{t}} \left[ \frac{ \left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}}{\frac{P_{t+i}}{P_{t}}} \right] - (1+\lambda_{p,t+i})\frac{MC_{t+i}}{P_{t+i}} \right\} = 0
\end{align}

\section*{Log-linearised DSGE model - Consumption}

Recall that:

\begin{align}
\lambda_{t} = \varepsilon_{t}^{B} \left( C_{t}^{\tau} - H_{t} \right)^{-\sigma_{c}}
\end{align}

Then from the household maximization problem it holds that:

\begin{align}
& \mathbb{E}_{t} \left[\beta \lambda_{t+1} \frac{R_{t}}{P_{t+1}} \right] = \frac{\lambda_{t}}{P_{t}} 
\end{align}

Then the left-hand side:

\begin{align}
& \mathbb{E}_{t} \left[\beta \lambda_{t+1} \frac{R_{t}}{P_{t+1}} \right]  \nonumber \\
& = \mathbb{E}_{t} \left[\beta \varepsilon_{t+1}^{B} \left( C_{t+1}^{\tau} - H_{t+1} \right)^{-\sigma_{c}} \frac{R_{t}}{P_{t+1}} \right] = \nonumber \\
& = \mathbb{E}_{t} \left[\beta \varepsilon_{t+1}^{B} \left( C_{t+1}^{\tau} - h C_{t} \right)^{-\sigma_{c}} \frac{R_{t}}{P_{t+1}} \right] \simeq \nonumber \\
& \simeq \mathbb{E}_{t} \left[\beta \varepsilon^{B} \left( C - hC \right)^{-\sigma_{c}} \frac{R}{P} +  \beta \varepsilon^{B} \left( C - hC \right)^{-\sigma_{c}} \frac{R}{P} \hat{\varepsilon}_{t+1}^{B} - \sigma_{c}\beta \varepsilon^{B} \left( C - hC \right)^{-\sigma_{c}-1} \left(C \hat{C}_{t+1} - h C \hat{C}_{t} \right) \frac{R}{P} + \right. \nonumber \\
& \left. \qquad + \beta \varepsilon^{B} \left( C - hC \right)^{-\sigma_{c}} \frac{R}{P} \hat{R}_{t} - \beta \varepsilon^{B} \left( C - hC \right)^{-\sigma_{c}} \frac{R}{P} \hat{P}_{t+1} \right] = \nonumber \\
& = \mathbb{E}_{t} \left[\beta \varepsilon^{B} \left( C - hC \right)^{-\sigma_{c}} \frac{R}{P} \cdot \left( 1 +  \hat{\varepsilon}_{t+1}^{B} - \sigma_{c} \left(C - hC \right)^{-1} \left(C \hat{C}_{t+1} - hC \hat{C}_{t} \right) + \hat{R}_{t} - \hat{P}_{t+1} \right) \right]
\end{align}

While the right-hand side:

\begin{align}
\frac{\lambda_{t}}{P_{t}} &= \varepsilon_{t}^{B} \left( C_{t}^{\tau} - H_{t} \right)^{-\sigma_{c}}  P_{t}^{-1} =  \varepsilon_{t}^{B} \left( C_{t}^{\tau} - h C_{t-1} \right)^{-\sigma_{c}}  P_{t}^{-1} \simeq \nonumber \\
& \simeq \varepsilon^{B} \left( C - h C  \right)^{-\sigma_{c}}  P^{-1} +  \varepsilon^{B} \left( C - h C  \right)^{-\sigma_{c}}  P^{-1} \hat{\varepsilon}_{t}^{B} + \nonumber \\
& \qquad - \sigma_{C} \varepsilon^{B} \left( C - h C  \right)^{-\sigma_{c}-1}  P^{-1} \left(C \hat{C}_{t} - h C \hat{C}_{t-1} \right) + \nonumber \\
& \qquad - \varepsilon^{B} \left( C - h C  \right)^{-\sigma_{c}}  P^{-1} \hat{P}_{t} = \nonumber \\
& = \varepsilon^{B} \left( C - h C  \right)^{-\sigma_{c}}  P^{-1} \left( 1 + \hat{\varepsilon}_{t}^{B} - \sigma_{C} \left( C - h C  \right)^{-1} \left(C \hat{C}_{t} - h C \hat{C}_{t-1} \right) -\hat{P}_{t}
\right)
\end{align}

Finally considering that in the steady-state:

\begin{align}
\beta R \left[ \varepsilon^{B} \left( C - h C  \right)^{-\sigma_{c}} \right] &= \varepsilon^{B} \left( C - h C  \right)^{-\sigma_{c}} \frac{P}{P} \nonumber \\
\iff & \beta = \frac{1}{R}
\end{align}

Putting both sides together yields:

\begin{align}
\hat{R}_{t} + \mathbb{E}_{t} \hat{\varepsilon}^{B}_{t+1} - \frac{\sigma_{c}}{1-h} \left( \mathbb{E}_{t}\hat{C}_{t+1} - h \hat{C}_{t} \right) = \hat{\varepsilon}^{B}_{t} - \frac{\sigma_{c}}{1-h} \left( \hat{C}_{t} - h \hat{C}_{t-1} \right) + \mathbb{E}_{t} \hat{\Pi}_{t+1}
\end{align}

from which one obtains the explicit expression for $\hat{C}_{t}$.

\section*{Log-linearised DSGE model - Investment}

Recall the optimal condition for investment:

\begin{align}
\frac{\partial L}{\partial I_{t}} &= \mathbb{E}_{t} \left[ -\beta^{t} \lambda_{t} - \beta^{t} \lambda_{t} Q_{t} \left(-1 + S\left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) + I_{t} \cdot S' \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \cdot \frac{\varepsilon_{t}^{I}}{I_{t-1}} \right) \right] + \nonumber \\
\qquad &- \mathbb{E}_{t} \left[ \beta^{t+1} \lambda_{t+1} Q_{t+1} \left(I_{t+1} \cdot S' \left(\frac{\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}} \right) \cdot \frac{-\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}^{2}} \right) \right] = 0 \nonumber \\
& \iff Q_{t}S' \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \left( \frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) - \beta \mathbb{E}_{t} \left[ Q_{t+1} \frac{\lambda_{t+1}}{\lambda_{t}} S' \left(\frac{\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}} \right) \frac{\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}}  \frac{I_{t+1}}{I_{t}} \right] + 1 = \nonumber \\
& \qquad \quad = Q_{t} \left( 1 - S \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \right)
\end{align}

Since in the steady state $\varepsilon^{I} = 1$ and $S(1) = S'(1) = 0$, it holds:

\begin{align}
QS' \left(\frac{\varepsilon^{I}I}{I} \right) \left( \frac{\varepsilon^{I}I}{I} \right) - \beta \left[ Q \frac{\lambda}{\lambda} S' \left(\frac{\varepsilon^{I}I}{I} \right) \frac{\varepsilon^{I}I}{I}  \frac{I}{I} \right] + 1 = Q \left( 1 - S \left(\frac{\varepsilon^{I}I}{I} \right) \right) \iff Q = 1
\end{align}

The approximation of the single terms yields:

\begin{align}
S' \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} &\simeq S'(1) \frac{\varepsilon^{I}I}{I} + \left[ S''(1) \left(\frac{I}{I} \right) \frac{\varepsilon^{I}I}{I} + S'(1) \frac{I}{I} \right] \left( \varepsilon_{t}^{I} - \varepsilon^{I} \right) + \nonumber \\
&  + \left[ S''(1) \left(\frac{\varepsilon^{I}}{I} \right) \frac{\varepsilon^{I}I}{I} + S'(1) \frac{\varepsilon^{I}}{I} \right] \left( I_{t} - I \right) + \nonumber \\
& + \left[ S''(1) \left(\frac{-\varepsilon^{I}I}{I^{2}} \right) \frac{\varepsilon^{I}I}{I} + S'(1) \frac{-\varepsilon^{I}I}{I^{2}} \right] \left( I_{t-1}- I \right) = \nonumber \\
& = S''(1) \left[ \hat{I}_{t} - \hat{I}_{t-1} + \hat{\varepsilon}_{t}^{I} \right]
\end{align}

Note that since $S'(1) = 0$ one does not need the approximation of $Q_{t}, Q_{t+1}, \lambda_{t}$ and $\lambda$.

\begin{align}
&\beta \mathbb{E}_{t} \left[ \frac{Q_{t+1}}{Q_{t}} \frac{\lambda_{t+1}}{\lambda_{t}} S' \left(\frac{\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}} \right) \frac{\varepsilon_{t+1}^{I}I_{t+1}}{I_{t}}  \frac{I_{t+1}}{I_{t}} \right] \simeq \nonumber \\
& \simeq \beta \frac{Q}{Q} \frac{\lambda}{\lambda} S' \left(\frac{\varepsilon^{I}I}{I} \right) \frac{\varepsilon^{I}I^{2}}{I^{2}} + \beta \frac{Q}{Q} \frac{\lambda}{\lambda} \left[ S''(1) \frac{\varepsilon^{I}}{I} \frac{\varepsilon^{I} I^{2}}{I^{2}} + 2S'(1) \frac{\varepsilon^{I} I}{I^{2}} \right] \left(\mathbb{E}_{t} I_{t+1} - I \right) + \nonumber \\
& \quad + \beta \frac{Q}{Q} \frac{\lambda}{\lambda} \left[ S''(1) \frac{-\varepsilon^{I}I}{I^{2}} \frac{\varepsilon^{I} I^{2}}{I^{2}} - 2S'(1) \frac{\varepsilon^{I} I^{2}}{I^{3}} \right] \left(I_{t} - I \right) + \nonumber \\
& \quad + \beta \frac{Q}{Q} \frac{\lambda}{\lambda} \left[ S''(1) \frac{I}{I} \frac{\varepsilon^{I} I^{2}}{I^{2}} + S'(1) \frac{I^{2}}{I^{2}} \right] \left(\mathbb{E}_{t} \varepsilon_{t+1}^{I} - \varepsilon^{I} \right) = \nonumber \\
& = \beta S''(1) \left[\mathbb{E}_{t} \hat{I}_{t+1} - \hat{I}_{t} + \mathbb{E}_{t} \varepsilon_{t+1}^{I} \right]
\end{align}

Since $Q = 1$:

\begin{align}
\frac{1}{Q_{t}} \simeq \frac{1}{Q} - \frac{1}{Q^{2}} \left(Q_{t} - Q \right).= 1 - \hat{Q}_{t}
\end{align}

Moreover:

\begin{align}
S \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \simeq S(1) + S'(1) \left[ \frac{I}{I} \left( \varepsilon_{t}^{I} - \varepsilon \right) + \frac{\varepsilon^{I}}{I} \left( I_{t}-I \right) - \frac{\varepsilon^{I}I}{I^{2}} \left( I_{t-1}-I \right) \right] = 0
\end{align}

Hence it follows:

\begin{align}
S''(1) \left[\hat{I}_{t} - \hat{I}_{t-1} + \varepsilon_{t}^{I} \right] - \beta S''(1) \left[\mathbb{E}_{t} \hat{I}_{t+1} - \hat{I}_{t} + \mathbb{E}_{t} \varepsilon_{t+1}^{I} \right] \simeq 1 - (1 - \hat{Q}_{t})
\end{align}

Thus:

\begin{align}
\hat{I}_{t} = \frac{1}{1+\beta} \hat{I}_{t-1} + \frac{\beta}{1+\beta} \mathbb{E}_{t} \hat{I}_{i+1} + \dfrac{1}{S''(1)(1+\beta)}\hat{Q}_{t} + \frac{\beta \mathbb{E}_{t} \hat{\varepsilon}_{t+1}^{I} - \hat{\varepsilon}_{t}^{I}}{1+\beta}
\end{align}

\section*{Log-linearised DSGE model - Q-equation}

Recall that 

\begin{align}
Q_{t} = \mathbb{E}_{t} \left[ \beta \frac{\lambda_{t+1}}{\lambda_{t}} \left( Q_{t+1}(1-\tau) + z_{t+1}r_{t+1}^{k} - \Psi(z_{t+1}) \right) \right] 
\end{align}

In the steady state $Q = 1$, $z = 1$ and $\Psi(1) = 0$, thus:

\begin{align}
Q &= \beta \frac{\lambda}{\lambda} \left( Q(1-\tau) + z r^{k} - \Psi(z) \right) \nonumber \\
& \iff 1 = \beta \left( 1 - \tau + r^{k} \right)
\end{align}

The linearization of the left side yields:

\begin{align}
Q_{t} \simeq 1 + \hat{Q}_{t}
\end{align}

Exploiting the fact that $r^{k} = \Psi'(z)$, the right side approximates by:

\begin{align}
&  \mathbb{E}_{t} \left[ \beta \frac{\lambda_{t+1}}{\lambda_{t}} \left( Q_{t+1}(1-\tau) + z_{t+1}r_{t+1}^{k} - \Psi(z_{t+1}) \right) \right]  \simeq \nonumber \\
& \simeq \beta \left( 1 - \tau + r^{k} \right) +  \beta \frac{1}{\lambda} \underbrace{\left[ Q(1-\tau) + zr^{k} - \Psi(z) \right] }_{\beta^{-1}} \left(\mathbb{E}_{t} \lambda_{t+1} - \lambda \right) + \nonumber \\
& \quad -  \beta \frac{\lambda}{\lambda^{2}} \underbrace{ \left[ Q(1-\tau) + zr^{k} - \Psi(z) \right] }_{\beta^{-1}}\left(\lambda_{t} - \lambda \right) + \nonumber \\
& \quad + \beta \frac{\lambda}{\lambda} \left(1-\tau \right)\left( \mathbb{E}_{t} Q_{t+1} - Q \right) + \beta \frac{\lambda}{\lambda} z \left(  \mathbb{E}_{t} r^{k}_{t+1} - r \right) + \nonumber \\
& \quad + \beta \frac{\lambda}{\lambda} \left( r^{k} - \Psi'(z) \right) \left( z_{t+1} - z \right) = \nonumber \\
& = 1 + \mathbb{E}_{t} \hat{\lambda}_{t+1} - \hat{\lambda}_{t} + \beta(1-\tau)\mathbb{E}_{t}\hat{Q}_{t+1} + \beta \mathbb{E}_{t} r^{k} \hat{r}_{t+1}
\end{align}

From the Euler equation:

\begin{align}
\mathbb{E}_{t}\hat{\lambda}_{t+1} - \hat{\lambda}_{t} = - \left( \hat{R}_{t} - \hat{\pi}_{t+1} \right)
\end{align}

Putting the above-mentioned conditions together:

\begin{align}
\hat{Q}_{t} = - \left( \hat{R}_{t} - \hat{\pi}_{t+1}  \right) +  \beta(1-\tau)\mathbb{E}_{t}\hat{Q}_{t+1} + \beta \mathbb{E}_{t} r^{k} \hat{r}_{t+1}
\end{align}

\section*{Log-linearised DSGE model - Capital accumulation}

Starting from:

\begin{align}
K_{t} = K_{t-1} (1 - \tau) + I_{t} \left[ 1 - S \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \right]
\end{align}

In the steady state it holds:

\begin{align}
K =  (1 - \tau)K + I \left[ 1 - \underbrace{S (1)}_{=0}] \right] \iff I = \tau K
\end{align}

The approximation of the left side yields:

\begin{align}
K_{t} = K + \left(K_{t} - K \right) \simeq K + \hat{K}_{t}K
\end{align}

Analogously for the right side:

\begin{align}
K_{t-1} (1 - \tau) + I_{t} \left[ 1 - S \left(\frac{\varepsilon_{t}^{I}I_{t}}{I_{t-1}} \right) \right] \simeq K(1 - \tau) + I + (1 - \tau) \left(K_{t-1} - K \right) + \left( I_{t} - I \right)
\end{align}

Considering the steady state relationships:

\begin{align}
&\hat{K}_{t}K = (1 - \tau)K \hat{K}_{t-1} + \hat{I}_{t}I = (1 - \tau)K \hat{K}_{t-1} + \hat{I}_{t}\tau K \nonumber \\
& \Rightarrow \hat{K}_{t} = (1 - \tau) \hat{K}_{t-1} + \hat{I}_{t-1}\tau
\end{align} 

\section*{Log-linearised DSGE model - Prices}

Starting from:

\begin{align} &\mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \frac{\lambda_{t+i}}{\lambda_{t}} y_{t+i}^{j} \left\{ \frac{\widetilde{p}_{t}}{P_{t}} \left[ \frac{ \left(\frac{P_{t+i-1}}{P_{t-1}} \right)^{\gamma_{p}}}{\frac{P_{t+i}}{P_{t}}} \right] - (1+\lambda_{p,t+i})\frac{MC_{t+i}}{P_{t+i}} \right\} = 0
\end{align}

Denote $\phi_{t} = \log(\widetilde{p}_{t})$, $p_{t} = \log(P_{t})$, $mc_{t} = \log \left(\frac{MC_{t}}{P_{t}} \right)$ and $ \mu = \log(1+\lambda)$. Then:

\begin{align} \label{Log-linearised DSGE model - Prices eq}
&\mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \frac{\lambda_{t+i}}{\lambda_{t}} y_{t+i}^{j} \left\{ e^{\phi_{t} - p_{t+i} + \gamma_{p} \left(p_{t+i-1} - p_{t-1} \right) } - e^{\mu + mc_{t+i}} \right\} = 0
\end{align}

In the steady state it holds $\phi_{t} = p_{t} = p $, thus:

\begin{align}
& e^{0} - e^{\mu + mc} = 0 \nonumber \\
& \iff mc = -\mu
\end{align}

Having noticed that the internal bracket of equation \ref{Log-linearised DSGE model - Prices eq}  is null in the steady state:

\begin{align} \label{Log-linearised DSGE model - Prices eq2}
\mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} &\beta^{i} \varsigma_{p}^{i} \frac{\lambda}{\lambda} y \left\{ e^{0} \left(\phi_{t} - p \right) + e^{0} (-1) \left(p_{t+i} - p \right) + e^{0} \left( p_{t+i-1} - p \right) - e^{0}\gamma_{p} \left(p_{t-1} - p \right) + \nonumber \right. \\
& \left. - e^{0} \left(mc_{t+i} - mc \right) \right\} = 0 \nonumber \\
& \Rightarrow \mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \phi_{t} = \mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \left[\underbrace{-mc}_{=\mu}  + mc_{t+i} + p_{t+i} - \gamma_{p} \left(p_{t+i-1} - p_{t-1} \right) \right] \nonumber \\
& \Rightarrow \frac{1}{1-\beta \varsigma_{p}} \phi_{t} = \frac{1}{1-\beta \varsigma_{p}} \mu + \mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \left[mc_{t+i} + p_{t+i} - \gamma_{p} \left(p_{t+i-1} - p_{t-1} \right) \right] \nonumber \\
& \Rightarrow \phi_{t} = \mu +  \left( 1-\beta \varsigma_{p} \right) \mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \left[mc_{t+i} + p_{t+i} - \gamma_{p} \left(p_{t+i-1} - p_{t-1} \right) \right] \nonumber \\
& \Rightarrow \hat{\phi}_{t} = \phi_{t} - p = \left( 1-\beta \varsigma_{p} \right) \mathbb{E}_{t}  \sum\limits_{i=0}^{\infty} \beta^{i} \varsigma_{p}^{i} \left[\hat{mc}_{t+i} + \hat{p}_{t+i} - \gamma_{p} \left(p_{t+i-1} - p_{t-1} \right) \right] \nonumber \\
& \Rightarrow \hat{\phi}_{t} - \beta \varsigma_{p} \mathbb{E}_{t}  \hat{\phi}_{t+1} = \left( 1 - \beta \varsigma_{p} \right) \left(\hat{mc}_{t} + \hat{p}_{t} \right) 
\end{align} 

Moreover, the linearization of the equation of motion of the price level yields:

\begin{align}
& \left( P_{t} \right)^{\frac{1}{-\lambda_{p,t}}} =  \varsigma_{p} \left[ P_{t-1}\left(\frac{P_{t-1}}{P_{t-2}}\right)^{\gamma_{p}} \right]^{\frac{-1}{\lambda_{p,t}}} + (1-\varsigma_{p}) \left(\widetilde{p}_{t} \right)^{\frac{-1}{\lambda_{p,t}}} \nonumber \\
& \Rightarrow 1 = \varsigma_{p} \left[ \Pi_{t-1}^{\gamma_{p}} \Pi_{t}^{-1} \right]^{\frac{-1}{\lambda_{p,t}}} + (1-\varsigma_{p}) \left(\frac{\widetilde{p}_{t}}{P_{t}} \right)^{\frac{-1}{\lambda_{p,t}}} \nonumber \\
& \Rightarrow 0 = \frac{-1}{\lambda_{p,t}} \left[ \Pi^{\gamma_{p}} \Pi^{-1} \right]^{\frac{-1}{\lambda_{p,t}}-1} \left[\gamma_{p} \pi^{\gamma_{p}-1} \left(\Pi_{t-1} - \Pi \right) - \Pi^{-2} \left(\Pi_{t} - \Pi \right)
\right] + \nonumber \\
& \qquad - \frac{-1}{\lambda_{p,t}} (1-\varsigma_{p}) \frac{P}{P}^{\frac{-1}{\lambda_{p,t}}-1} \frac{1}{P} \left[ \left(\widetilde{p}_{t} - P \right) \left( P_{t} - P \right) \right]  \nonumber \\
& \Rightarrow \hat{\phi}_{t} - \hat{p}_{t} = \frac{\varsigma_{p}}{1-\varsigma_{p}} \left( \hat{\pi}_{t} - \gamma_{p} \hat{\pi}_{t-1} \right)
\end{align}

The substitution into \ref{Log-linearised DSGE model - Prices eq2} yields the new-keynesian Phillips curve:

\begin{align}
\hat{\pi}_{t} = \frac{\beta}{1+\beta \gamma_{p}} \mathbb{E}_{t} \hat{\pi}_{t+1} + \frac{\gamma_{p}}{1+\beta \gamma_{p}} \hat{\pi}_{t-1} + \frac{\left( 1 - \beta \varsigma_{p} \right) \left( 1 - \varsigma_{p} \right)}{\left(1+\beta \gamma_{p} \right) \varsigma_{p}} \left( \underbrace{\alpha \hat{r}^{k}_{t} + (1-\alpha)\hat{w}_{t} - \hat{\varepsilon}_{t}^{a} + \eta_{t}^{p} }_{\hat{mc}_{t}} \right)
\end{align}

\section*{Log-linearised DSGE model - Labor demand}

Starting with:

\begin{align}
\frac{W_{t} L_{t}}{r_{t}^{k}z_{t}K_{t-1}} = \frac{1-\alpha}{\alpha}
\end{align}

In the steady state it holds:

\begin{align}
\frac{WL}{r^{k}zK} = \frac{1-\alpha}{\alpha}
\end{align}

Therefore:

\begin{align}
\log(L_{t}) - \log(L) &= \log(r_{t}^{k}) - \log(r^{k}) - \left[ \left( \log(W_{t}) + \log(P_{t}) \right) - \left( \log(W) - \log(P_{t}) \right) \right] + \nonumber \\
& + \log(z_{t}) - \log(z) + \log(K_{t-1}) - \log(K)
\end{align}

Hence:

\begin{align}
\hat{L}_{t} = - \hat{w}_{t} + \hat{r}^{k}_{t} + \hat{z}_{t} + \hat{K}_{t-1}
\end{align}

On the other hand:

\begin{align}
r_{t}^{k} = \Psi'(z_{t}) \Rightarrow r^{k} + r^{k} \hat{r}^{k}_{t} = \Psi'(z) + \Psi''(z)z_{t}\hat{z}_{t} \simeq \Psi'(z_{t}) 
\end{align}

But simultaneously:

\begin{align}
r^{k} &= \Psi'(z) \\
z &= 1
\end{align}

Thus:

\begin{align}
\hat{r}^{k}_{t}& \Psi'(1) =  \Psi''(1)\hat{z}_{t} \nonumber \\
& \Rightarrow \hat{z}_{t}  = \frac{\Psi'(1)}{\Psi''(1)} \hat{r}^{k}_{t}
\end{align}

It follows:

\begin{align}
\hat{L}_{t} = - \hat{w}_{t} + \left( 1 + \frac{\Psi'(1)}{\Psi''(1)} \right) \hat{r}^{k}_{t} + \hat{K}_{t-1}
\end{align}

\newpage

\chapter*{Appendix B} \label{Appendix B}
\addcontentsline{toc}{chapter}{Appendix B}

\lstinputlisting{SmetsWouters.mod}

\chapter*{Appendix C} \label{Appendix C}
\addcontentsline{toc}{chapter}{Appendix C}

\section*{DSGE estimation}

\lstinputlisting{DSGE_PyTorch.py}

\newpage

\section*{Loss function}

\lstinputlisting{Loss_function_PyTorch.py}

\newpage

\section*{DSGE solution}

\lstinputlisting{Solution_function_PyTorch.py}

\newpage

\section*{Kalman filter}

\lstinputlisting{Kalman_PyTorch.py}

\newpage

\section*{Linear Time Iteration algorithm}

\lstinputlisting{Linear_Time_Iteration_PyTorch.py}

\end{document}