# Methodology description

All the following methodology is based on the [New Introduction To Multiple Time Series Analysis][Helmut Lütkepohl] by Helmut Lütkepohl.

## Problem description {#Problem-description}

It is assumed that a K-dimensional multiple time series: $y_1,...,y_T$ with $y_t = (y_{1t},\ldots,y_{Kt})$ is available that is known to be generated by a stationary, stable _VAR(p)_ process:
\[
y_{t} =ν+A_{1} y_{t−1} +···+A_{p} y_{t−p} +u_{t}
\]

All symbols have their usual meanings, that is:

 * $ν = (ν_1,\ldots, ν_K)$ is a (K × 1) vector of intercept terms
 * $A_i$ are (K × K) coefficient matrices
 * $u_t$ is white noise with nonsingular covariance matrix $Σ_u$

 The coefficients $ν, A_1, \ldots , A_p$, and $Σ_u$ are assumed to be unknown in the following. The time series data will be used to estimate the coefficients.

 Furthermore, we define:

\[\begin{aligned}
Y &:= (y_1,...,y_T) \qquad &(K×T) \\
B &:= (ν, A_1,\ldots, A_p) \quad &(K×(Kp + 1)) \\
Z_t &: = \begin{bmatrix}
1 \\
y_{t} \\
\vdots \\
y_{t−p+1}
\end{bmatrix} \qquad &((Kp+1)×1)\\
Z &:=(Z_0,...,Z_{T−1}) \qquad &((Kp+1)×T) \\
U &:= (u_1,...,u_T) \qquad &(K×T)
\end{aligned}\]

## Libraries import and data simulation

```python
# ## Libraries

import torch
from torch import mm, t, inverse, diag, exp, reshape, empty, log, det, trace
from torch.autograd import Variable
# import matplotlib.pyplot as plt
from plotly import tools
from plotly.offline import init_notebook_mode, iplot
import plotly.graph_objs as go
import plotly.io as pio

# Set seed

torch.manual_seed(123)

# Templates for graphs

pio.templates.default = 'plotly_dark'
init_notebook_mode(connected=True)
```

In the first approach the four time series were simulated, each containing 1000 observations.

```python
# Simulation of stationary time-series

# Time-series dimension of simulation

nsample = 1000
nvar = 4

# Create null data frames for storing data

df = empty((nsample, nvar))
rho = empty((nvar))

# Simulation of processes

# Start values

for i in range(0, df.shape[1]):
    df[0, i] = torch.rand(1)

# Rho parameters, smaller than one in absolute value
for i in range(0, df.shape[1]):
    rho[i] = torch.rand(1)

# Create the AR(1) processes
for i in range(1, df.shape[0]):
    for j in range(0, df.shape[1]):
        df[i, j] = rho[j] * df[i - 1, j] + torch.randn(1)

print(df)
```

The time series simulation results in:

@import "Graphs/Simulated_time_series.png"

## VAR class

The VAR class has been created.

```python
class VAR:
    """
    **** VECTOR AUTOREGRESSION (VAR) MODELS ****
    ----------
    Parameters
    data : np.array
        Field to specify the time series data that will be used.
    lags : int
        Field to specify how many lag terms the model will have.
    integ : int (default : 0)
        Specifies how many time to difference the dependent variables.
    target : str (pd.DataFrame) or int (np.array) (default : None)
        By default, all columns will be selected as the dependent variables.
    """

    def __init__(self, data, lags, target=None, integ=0):

        # Latent Variables
        self.lags = lags
        self.integ = integ
        self.target = target
        self.model_name = "VAR(" + str(self.lags) + ")"

        # Format the dependant variables
        self.data = data
```

It allows for differencing the input data in order to obtain stationary time series.

```python
# Format the independent variables

    def diff(x, n):
        """ Calculate the n-th order discrete difference
        """
        new_torch = torch.zeros(list(x.shape)[0] - n)
        if n == 0:
            new_torch = x
        else:
            for i in range(list(x.shape)[0] - n):
                new_torch[i] = x[i] - x[i + n]
        return new_torch

    # Difference data

    self.data = t(torch.stack([diff(i, self.integ) for i in t(self.data)]))
    self.T = self.data.shape[0]
    self.ylen = self.data.shape[1]
```

Then the variables Y and Z are created, following the definitions from the [Problem description](#Problem-description) section.

```python
"""
    Y : torch.array
        Contains the length-adjusted time series (accounting for lags)
    """

    self.Y = t(self.data[self.lags:, ])

def _design(self):
    """ Creates a design matrix
    Z : np.array
    """

    Z = torch.ones(((self.ylen * self.lags + 1), (self.T - self.lags)))

    row_count = 1
    for lag in range(1, self.lags + 1):
        for reg in range(self.ylen):
            Z[row_count, :] = self.data[:, reg][self.lags - lag:-lag]
            row_count += 1
    return(Z)
```

## Multivariate OLS

Solving the classical OLS problem yields the following estimator:

\[ \hat{B} = YZ'(ZZ')^{-1}\]

```python
def OLS(self):
      """ Creates OLS coefficient matrix
      ----------
      Parameters:
      NULL
      ----------
      Returns
      The coefficient matrix B
      """

      Z = self._design()
      return mm(mm(self.Y, t(Z)), inverse(mm(Z, t(Z))))
```

The results are as follows:

'A Constant': tensor(-0.0388),
 'A AR(1)': tensor(0.0713),
 'B to A AR(1)': tensor(0.0115),
 'C to A AR(1)': tensor(-0.0070),
 'D to A AR(1)': tensor(0.0085),
 'B Constant': tensor(0.0366),
 'B AR(1)': tensor(0.0012),
 'A to B AR(1)': tensor(0.8692),
 'C to B AR(1)': tensor(0.0013),
 'D to B AR(1)': tensor(0.0331),
 'C Constant': tensor(0.0340),
 'C AR(1)': tensor(0.0163),
 'A to C AR(1)': tensor(-0.0050),
 'B to C AR(1)': tensor(0.1362),
 'D to C AR(1)': tensor(0.0288),
 'D Constant': tensor(-0.0564),
 'D AR(1)': tensor(0.0235),
 'A to D AR(1)': tensor(-0.0032),
 'B to D AR(1)': tensor(-0.0432),
 'C to D AR(1)': tensor(0.1021)

 ## Maximum likelihood estimation

Define a _VAR(p)_ model in mean-adjusted form:
\[
(y_t −μ)=A_1(y_{t−1} −μ)+···+A_p(y_{t−p} −μ)+u_t
\]

Moreover let:

\[\begin{aligned}
Y^0 &:= (y_1−μ,...,y_T−μ) \qquad &(K×T) \\
A &:= (A_1,\ldots, A_p) \quad &(K×Kp) \\
Y_t^0 &: = \begin{bmatrix}
y_{t}−μ \\
\vdots \\
y_{t−p+1}−μ
\end{bmatrix} \qquad &(Kp×1)\\
X &:=(Y_0^0,...,Y_{T−1}^0) \qquad &(Kp×T) \\
\end{aligned}\]

Then the model can be rewritten as:
\[
Y_0 =AX+U
\]

Multivariate LS estimation of this model form is straightforward if the mean vector μ is known:
\[ \hat{A} = Y^0X'(XX')^{-1} \]

Assuming that the distribution of the process is known, maximum likelihood (ML) estimation is an alternative to LS estimation. We will consider ML estimation under the assumption that the _VAR(p)_ process $y_t$ is Gaussian.

Then the log-likelihood function takes form:

\[
l = −\dfrac{KT}{2}ln(2 \pi)− \dfrac{T}{2} ln|\Sigma_u| − \dfrac{1}{2} tr[(Y^0 − AX)' \Sigma_u^{-1}  (Y^0 −AX)]
\]

```python
def MLE(self):
      """ Creates MLE coefficient matrix
      ----------
      Parameters:
      NULL
      ----------
      Returns
      The coefficient matrix MLE
      ----------
      It is based on the assumption of normality of errors
      """

      par = Variable(torch.rand(self.lags * (self.ylen**2)
                                + self.ylen + self.ylen), requires_grad=True)

      coef = reshape(par[0:self.lags * self.ylen**2],
                     (self.ylen, self.lags * self.ylen))
      coef_mean = par[self.lags * self.ylen **
                      2:self.lags * self.ylen**2 + self.ylen]
      coef_var = diag(exp(par[self.lags * self.ylen**2 + self.ylen:]))
      Z = self._design()[1:]

      Y_0 = t(t(self.Y) - coef_mean)
      Z_0 = t(t(Z) - coef_mean.repeat(self.lags))

      learning_rate = 1e-5
      n_iter = 1000

      optimizer = torch.optim.SGD(params=[par], lr=learning_rate)

      def closure():
          # Before the backward pass, use the optimizer object to zero all of the
          # gradients for the Tensors it will update (which are the learnable weights
          # of the model)
          optimizer.zero_grad()

          # Without a constant term in the likelihood function:

          loss = -(-.5 * self.Y.shape[1] * log(torch.abs(det(coef_var))) - .5 * trace(
              mm(mm(t(Y_0 - mm(coef, Z_0)), inverse(coef_var)), Y_0 - mm(coef, Z_0))))

          # Backward pass: compute gradient of the loss with respect to model parameters

          loss.backward(retain_graph=True)

          return loss, par

      # Calling the step function on an Optimizer makes an update to its parameters

      loss_vector = torch.zeros(n_iter)
      par_vector = torch.zeros(
          (n_iter, self.lags * (self.ylen**2) + self.ylen + self.ylen))

      for i in range(n_iter):
          optimizer.step(closure)
          loss_vector[i], par_vector[i, :] = closure()

      return(par, loss_vector, par_vector)
```

The results are as follows:
'A AR(1)': tensor(0.1660, grad_fn=\<SelectBackward>),
 'B to A AR(1)': tensor(0.0391, grad_fn=\<SelectBackward>),
 'C to A AR(1)': tensor(0.1358, grad_fn=\<SelectBackward>),
 'D to A AR(1)': tensor(0.1678, grad_fn=\<SelectBackward>),
 'B AR(1)': tensor(0.0120, grad_fn=\<SelectBackward>),
 'A to B AR(1)': tensor(0.8724, grad_fn=\<SelectBackward>),
 'C to B AR(1)': tensor(0.0176, grad_fn=\<SelectBackward>),
 'D to B AR(1)': tensor(0.0513, grad_fn=\<SelectBackward>),
 'C AR(1)': tensor(0.1633, grad_fn=\<SelectBackward>),
 'A to C AR(1)': tensor(0.0379, grad_fn=\<SelectBackward>),
 'B to C AR(1)': tensor(0.3580, grad_fn=\<SelectBackward>),
 'D to C AR(1)': tensor(0.2763, grad_fn=\<SelectBackward>),
 'D AR(1)': tensor(0.1858, grad_fn=\<SelectBackward>),
 'A to D AR(1)': tensor(0.0441, grad_fn=\<SelectBackward>),
 'B to D AR(1)': tensor(0.2017, grad_fn=\<SelectBackward>),
 'C to D AR(1)': tensor(0.3753, grad_fn=\<SelectBackward>),
 'A mean': tensor(-40.6723, grad_fn=\<SelectBackward>),
 'B mean': tensor(17.7742, grad_fn=\<SelectBackward>),
 'C mean': tensor(-25.6389, grad_fn=\<SelectBackward>),
 'D mean': tensor(-69.6246, grad_fn=\<SelectBackward>),
 'logVar(A)': tensor(-52.2905, grad_fn=\<SelectBackward>),
 'logVar(B)': tensor(-88.9628, grad_fn=\<SelectBackward>),
 'logVar(C)': tensor(-159.2514, grad_fn=\<SelectBackward>),
 'logVar(D)': tensor(-48.6154, grad_fn=\<SelectBackward>)


Finally the graphs of the loss function per iteration step were generated, as well as the similar graphs for parameters changes.

@import "Graphs/Loss function per iteration.png"

 [Helmut Lütkepohl]: http://www.gbv.de/dms/mpib-toc/636851273.pdf
